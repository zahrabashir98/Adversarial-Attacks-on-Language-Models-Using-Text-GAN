{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of code e tamiz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ5ucqgr4DPf",
        "outputId": "fdcc3d70-3202-4a1d-af01-3c05456621b7"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkAjf4BNkzib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8d2628-4609-4de3-ec2a-242dbac2029e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MlxH3fqxRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b10536b-6431-4c86-ecaa-b6679cd1f55b"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# from model import VAE\n",
        "import keras\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from keras import objectives, backend as K\n",
        "import pickle\n",
        "\n",
        "\n",
        "disable_eager_execution()\n",
        "max_length = 200\n",
        "\n",
        "\n",
        "def read_vocab(path):\n",
        "    output = open(path, 'rb')\n",
        "    vocab = pickle.load(output)\n",
        "    output.close()\n",
        "    return vocab\n",
        "glove = read_vocab(\"/content/drive/MyDrive/MLprj/test2000\")\n",
        "\n",
        "def get_latent(data):\n",
        "  print(\"inja\")\n",
        "  print(data.shape)\n",
        "  reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Copy of modell.h5\", compile=False)\n",
        "\n",
        "  print(\"model ro khoond\")\n",
        "  def custom_loss():\n",
        "    def vae_loss(x, x_decoded_mean):\n",
        "          x = K.flatten(x)\n",
        "          x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "          xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
        "          return xent_loss \n",
        "    return vae_loss\n",
        "\n",
        "  vae_loss = custom_loss()\n",
        "\n",
        "  reconstructed_model.compile(optimizer='Adam',\n",
        "                              loss=custom_loss(), \n",
        "                                  metrics=['accuracy'])\n",
        "  \n",
        "  input_layer = reconstructed_model.layers[0]\n",
        "  encoder = reconstructed_model.get_layer(\"dense_2\")\n",
        "  print(\"reconstruct\")\n",
        "  # decoder = reconstructed_model.get_layer(\"decoded_mean\")\n",
        "\n",
        "  get_encoder_output = K.function([input_layer.input],[encoder.output])\n",
        "  print(\"aa\")\n",
        "  get_encoder_output = get_encoder_output([data])[0]\n",
        "  print(\"bbb\")\n",
        "  # get_decoder_output = K.function([encoder.output],[decoder.output])\n",
        "  # get_decoder_output = get_decoder_output([data])[0]\n",
        "  print(\"SHAPE:\",get_encoder_output.shape)\n",
        "  return get_encoder_output\n",
        "\n",
        "words_list = []\n",
        "vectors = []\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "for i in range(3, 2000): \n",
        "    word = reverse_word_index.get(i-3, '?')\n",
        "    vectors.append(glove[word])\n",
        "    words_list.append(word)\n",
        "\n",
        "def print_closest_words(vec):\n",
        "    closest_dist = 10000000\n",
        "    closest_word = -100\n",
        "    counter = 0 \n",
        "    for v in vectors: \n",
        "        dist = torch.norm(v - vec)\n",
        "        if closest_dist >= dist: \n",
        "            closest_word = counter \n",
        "            closest_dist = dist\n",
        "\n",
        "        counter += 1\n",
        "    return words_list[closest_word]\n",
        "\n",
        "\n",
        "def get_decoded(data):\n",
        "  # print(\"Latent SENTENCE\", data[0][0:10])\n",
        "\n",
        "  all = []\n",
        "  for sentence in data:\n",
        "    count = 0\n",
        "    decoded = []\n",
        "    for word in sentence:\n",
        "\n",
        "      decoded.append(print_closest_words(word))\n",
        "      # count += 1\n",
        "    all.append(decoded)\n",
        "  \n",
        "  print(\"bade decode\")\n",
        "  return np.array(all)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-vhcj5WCew"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def save_vocab(vocab, path):\n",
        "    with open(path, 'w+') as f:     \n",
        "        for token, index in vocab.stoi.items():\n",
        "            f.write(f'{index}\\t{token}')\n",
        "\n",
        "def read_vocab(path):\n",
        "    vocab = dict()\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            index, token = line.split('\\t')\n",
        "            vocab[token] = int(index)\n",
        "    return vocab\n",
        "    \n",
        "def get_embedding(dataset):\n",
        "    # The first time you run this will download a ~823MB file\n",
        "          # embedding size = 100\n",
        "    encoded = []\n",
        "    # print(\"DATASET[0]:\", dataset[0])\n",
        "    for sentence in dataset:\n",
        "        encoded.append(np.array([np.array(glove[word]) for word in sentence]))\n",
        "    # print(\"ENCODED[0]\", encoded[0][0:10])\n",
        "    return np.array(encoded)\n",
        "    \n",
        "# get_embedding()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNjlWv8QFVr1",
        "outputId": "b61da00d-13fd-4637-9227-f81f618f5dc8"
      },
      "source": [
        "!pip3 install hickle"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hickle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/ee/7f442cb653c22f6f1d9de922919be58d81bc8a09ec4f6d886ce447683596/hickle-4.0.4-py3-none-any.whl (49kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 20kB 29.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 40kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.19.5)\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (0.3.3)\n",
            "Requirement already satisfied: h5py<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (2.10.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.15.0)\n",
            "Installing collected packages: hickle\n",
            "Successfully installed hickle-4.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZbDfgxvXJx"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBl_Snh3tVx-",
        "outputId": "bcacb8ab-d221-4aa1-f9c6-fd8755190f67"
      },
      "source": [
        "import hickle as hkl\n",
        "def load_dataset():\n",
        "  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 2000) \n",
        "  print(train_data.shape)\n",
        "  print(test_data.shape)\n",
        "  # gan_test_data = test_data[0:2000]\n",
        "  # gan_test_labels = test_labels[0:2000]\n",
        "  \n",
        "  array_hkl = hkl.load('/content/drive/MyDrive/MLprj/data_new2000.hkl')\n",
        "  X_train = array_hkl['xtest']\n",
        "  gan_test_labels = array_hkl['ytest']\n",
        "  word_index = imdb.get_word_index()\n",
        "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "  gan_test_data = []\n",
        "  print(X_train.shape)\n",
        "  for i  in  range(len(X_train)):\n",
        "    decoded_review = [reverse_word_index.get(i - 3, '?') for i in X_train[i]]\n",
        "    gan_test_data.append(decoded_review)\n",
        "\n",
        "  niave_total_data = np.concatenate((train_data, test_labels[10000:]), axis = 0)\n",
        "  naive_total_labels = np.concatenate((train_labels, test_labels[10000:]), axis = 0)\n",
        "\n",
        "  return gan_test_data, gan_test_labels, niave_total_data, naive_total_labels\n",
        "\n",
        "gan_test_data, gan_test_labels, niave_total_data, naive_total_labels = load_dataset()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "(25000,)\n",
            "(25000,)\n",
            "(10000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oQZZWjoS7Gk"
      },
      "source": [
        "# Niave Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihg3-CzGZAQl",
        "outputId": "dba9e7b7-c321-494c-c5ff-ef32b3799874"
      },
      "source": [
        "# !pip install -U scikit-learn\n",
        "!python -m pip install scikit-learn --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afm6y_UZS9ND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b36ccce-0550-4819-b3d4-f719dcc49fad"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "filename = '/content/drive/MyDrive/MLprj/naive_model.sav'\n",
        "\n",
        "def preprocess(data):\n",
        "\n",
        "      truncated_data = []  \n",
        "      MAX_LENGTH= 200\n",
        "      for row in data:\n",
        "          if type(row) == type([1,1]):\n",
        "              truncated_data.append(row[:MAX_LENGTH])\n",
        "      data = np.array(truncated_data)\n",
        "      print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      print(data[0])\n",
        "\n",
        "\n",
        "      def vectorize_sequences(sequences, dimension=2000):\n",
        "          results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "          for i,sequence in enumerate(sequences):\n",
        "              results[i,sequence] = 1   \n",
        "                          # Sets specific indices of results[i] to 1s\n",
        "          return results\n",
        "\n",
        "      # Vectorize training Data\n",
        "      print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      print(data[0])\n",
        "      \n",
        "      data = vectorize_sequences(data)\n",
        "      print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      print(data[0])\n",
        "\n",
        "\n",
        "      # MAX_LENGTH= 2000\n",
        "      # data = pad_sequences(data, maxlen=MAX_LENGTH)\n",
        "      # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      # print(data[0])\n",
        "      return data\n",
        "\n",
        "\n",
        "def train_naive():\n",
        "    X = preprocess(niave_total_data)\n",
        "    \n",
        "\n",
        "    y = naive_total_labels\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    print(\"\\n\")\n",
        "    kf = KFold(n_splits=10, random_state=None, shuffle = False) \n",
        "    # X_train , Y_train, X_test, Y_test = load_data_Naive()\n",
        "    # X = np.concatenate((X_train, X_test), axis=0)\n",
        "    # y = np.concatenate((Y_train, Y_test), axis=0)\n",
        "    # i = 0\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    best_acc = 0\n",
        "    for train_index, test_index in kf.split(X):\n",
        "          # print(\"Train:\", train_index, \"Validation:\",test_index)\n",
        "          x_train, x_test = X[train_index], X[test_index] \n",
        "          y_train, y_test = y[train_index], y[test_index]\n",
        "          bnb = BernoulliNB(binarize=0.0)\n",
        "          bnb.fit(x_train, y_train)\n",
        "          y_pred = bnb.predict(x_train)\n",
        "          train_acc = accuracy_score(y_train, y_pred)\n",
        "          print(\"accuracy:\", train_acc)\n",
        "          train_accs.append(train_acc)\n",
        "          test_acc = bnb.score(x_test, y_test)\n",
        "          print(\"SCORE\",test_acc)\n",
        "          test_accs.append(test_acc)\n",
        "          if train_acc > best_acc:\n",
        "            print(\"THIS IS the max acc\", train_acc)\n",
        "            best_acc = train_acc\n",
        "            pickle.dump(bnb, open(filename, 'wb'))\n",
        "          print(\"*************\\n\")\n",
        "    print(\"Average Train Accuracy:\", np.mean(np.array(train_accs)))\n",
        "    print(\"Average Test Score:\", np.mean(np.array(test_accs)))\n",
        "\n",
        "train_naive()\n",
        "\n",
        "\n",
        "def get_naive_model():\n",
        "    # load the model from disk\n",
        "    loaded_model = pickle.load(open(filename, 'rb'))\n",
        "    return loaded_model\n",
        "\n",
        "get_naive_model()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> (25000,)\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16]\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> (25000,)\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16]\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> (25000, 2000)\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "(25000, 2000)\n",
            "(40000,)\n",
            "\n",
            "\n",
            "accuracy: 0.8259555555555556\n",
            "SCORE 0.806\n",
            "THIS IS the max acc 0.8259555555555556\n",
            "*************\n",
            "\n",
            "accuracy: 0.8250222222222222\n",
            "SCORE 0.8252\n",
            "*************\n",
            "\n",
            "accuracy: 0.8262666666666667\n",
            "SCORE 0.8256\n",
            "THIS IS the max acc 0.8262666666666667\n",
            "*************\n",
            "\n",
            "accuracy: 0.8261333333333334\n",
            "SCORE 0.8136\n",
            "*************\n",
            "\n",
            "accuracy: 0.8264888888888889\n",
            "SCORE 0.8224\n",
            "THIS IS the max acc 0.8264888888888889\n",
            "*************\n",
            "\n",
            "accuracy: 0.8264888888888889\n",
            "SCORE 0.8212\n",
            "*************\n",
            "\n",
            "accuracy: 0.8270666666666666\n",
            "SCORE 0.8064\n",
            "THIS IS the max acc 0.8270666666666666\n",
            "*************\n",
            "\n",
            "accuracy: 0.8252\n",
            "SCORE 0.824\n",
            "*************\n",
            "\n",
            "accuracy: 0.826\n",
            "SCORE 0.8196\n",
            "*************\n",
            "\n",
            "accuracy: 0.8255555555555556\n",
            "SCORE 0.8204\n",
            "*************\n",
            "\n",
            "Average Train Accuracy: 0.8260177777777779\n",
            "Average Test Score: 0.8184400000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkC8DUp1TCwY"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYCRPTGQCVn3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "febd9c16-e84e-446f-9c9c-78829701c6f2"
      },
      "source": [
        "import os\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization, LSTM, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import hickle as hkl\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.vector_size = 200\n",
        "        self.txt_shape = (self.vector_size,50,)\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "        self.batch_size = 10\n",
        "        self.generator = self.build_generator()\n",
        "        self.total_loss = 200\n",
        "        self.array = np.zeros( shape = (self.batch_size,1))\n",
        "        self.array[0] += self.total_loss\n",
        "\n",
        "        # z = Input(shape=(self.latent_dim,))\n",
        "        data = Input(shape=self.txt_shape)\n",
        "        noise = self.generator(data)\n",
        "        noise = tf.expand_dims(noise, axis=2)\n",
        "        noisy_img = data + 6 * noise\n",
        "        # print(noisy_img.shape)\n",
        "        # noisy_image = K.sum(noise, data)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "        validity = self.discriminator(noisy_img)\n",
        "        self.combined = Model(data, validity)\n",
        "        self.combined.compile(loss= self.get_total_loss, optimizer=optimizer)\n",
        "\n",
        "        self.founded_examples = []\n",
        "  \n",
        "    def get_total_loss(self, fake, validity):\n",
        "      ep = 0.000001\n",
        "      t_loss = K.sum((-1)*(fake * K.log(validity) + (1 - fake) * K.log(1 - validity))) + self.array[0]\n",
        "\n",
        "      return t_loss\n",
        "\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        array_hkl = hkl.load('/content/drive/MyDrive/MLprj/data2000.hkl')\n",
        "        X_train = array_hkl['xtrain']\n",
        "        X_test = array_hkl['xtest']\n",
        "        y_train = array_hkl['ytrain']\n",
        "        y_test = array_hkl['ytest']\n",
        "\n",
        "        word_index = imdb.get_word_index()\n",
        "        # step 2: reverse word index to map integer indexes to their respective words\n",
        "        reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "        train_sentences = []\n",
        "        print(X_train.shape)\n",
        "        for i  in  range(len(X_train)):\n",
        "          decoded_review = [reverse_word_index.get(i - 3, '?') for i in X_train[i]]\n",
        "          train_sentences.append(decoded_review)\n",
        "        return train_sentences, y_train \n",
        "\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(50))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.vector_size, activation='tanh'))\n",
        "        # model.summary()\n",
        "        data = Input(shape = self.txt_shape)\n",
        "        noise = model(data)\n",
        "\n",
        "        return Model(data, noise)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        LSTM_units = 128\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(LSTM_units))\n",
        "        model.add(Dense(100))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(10))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # model.summary()\n",
        "        img = Input(shape = self.txt_shape)\n",
        "        validity = model(img)\n",
        "        return Model(img, validity)\n",
        "        \n",
        "\n",
        "    def create_model_checkpoint(self, dir, model_name, epoch):\n",
        "      filepath = dir + '/' + \\\n",
        "                model_name + \"-{epoch:%d}.h5\"%epoch\n",
        "      directory = os.path.dirname(filepath)\n",
        "\n",
        "      try:\n",
        "          os.stat(directory)\n",
        "      except:\n",
        "          os.mkdir(directory)\n",
        "\n",
        "      checkpointer = ModelCheckpoint(filepath=filepath,\n",
        "                                    verbose=1,\n",
        "                                    save_best_only=False)\n",
        "\n",
        "      return checkpointer\n",
        "\n",
        "\n",
        "    def find_good_review(self, data_array, label):\n",
        "        find = True\n",
        "        number = 200\n",
        "        if find: \n",
        "            newdata = []\n",
        "            newlabel = []\n",
        "            for i in range(len(data_array)):\n",
        "                sentence_started = False\n",
        "                count = 0\n",
        "                for word in data_array[i]:\n",
        "                    if word != \"?\" and (not sentence_started):\n",
        "                          sentence_started = True\n",
        "                    if sentence_started and word == \"?\":\n",
        "                          count += 1\n",
        "                    if count >6: \n",
        "                          break \n",
        "                if count <6: \n",
        "                    newdata.append(data_array[i])\n",
        "                    newlabel.append(label[i])\n",
        "                    number -= 1\n",
        "                if number <=0: \n",
        "                    return  newdata, np.array(newlabel)\n",
        "\n",
        "            print(\"Only found: \", 200 -number)\n",
        "            return  newdata, np.array(newlabel)\n",
        "\n",
        "        return np.array(data_array)[0:number], np.array(label)[0:number]\n",
        "\n",
        "    def pre_process(self, X_test):\n",
        "      new_test = []\n",
        "      tmp = []\n",
        "      for sentence in X_test:\n",
        "        for word in sentence:\n",
        "          if word == '?':\n",
        "            tmp.append(0)\n",
        "          else:\n",
        "            tmp.append(word_index[word])\n",
        "        new_test.append(list(tmp))\n",
        "      x_test = np.array(new_test)\n",
        "      \n",
        "      def vectorize_sequences(sequences, dimension=2000):\n",
        "          results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "          for i,sequence in enumerate(sequences):\n",
        "              results[i,sequence] = 1                        # Sets specific indices of results[i] to 1s\n",
        "          return results\n",
        "\n",
        "      MAX_LENGTH= 200\n",
        "      # Vectorize testing Data\n",
        "\n",
        "      truncated_data = []  \n",
        "\n",
        "      for row in x_test:\n",
        "          if type(row) == type([1,1]):\n",
        "              truncated_data.append(row[:MAX_LENGTH])\n",
        "      x_test = np.array(truncated_data)\n",
        "\n",
        "      X_test = vectorize_sequences(x_test)\n",
        "      # X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
        "     \n",
        "      return X_test\n",
        "\n",
        "    def naive(self, bnb, X_test, Y_test):\n",
        "  \n",
        "      X_test = self.pre_process(X_test)\n",
        "      print(\"SCORE\",bnb.score(X_test, Y_test))\n",
        "      \n",
        "      def CrossEntropy(pred, y):\n",
        "\n",
        "              ep = 0.000001\n",
        "              pos = np.sum(y * -np.log(1 - pred + ep))\n",
        "              neg = np.sum((1-y) * -np.log(pred + ep))\n",
        "              # print(type(pos))\n",
        "              neg + pos\n",
        "            \n",
        "              return neg + pos\n",
        "\n",
        "\n",
        "      def CrossEntropy_Naive(pred, y):\n",
        "        ep = 0.000001\n",
        "        pos = np.sum(y * -np.log(pred + ep))\n",
        "        neg = np.sum((1-y) * -np.log(1-pred + ep))\n",
        "       \n",
        "        neg + pos\n",
        "        return neg + pos\n",
        "\n",
        "      predicts = bnb.predict(X_test)\n",
        "      loss = CrossEntropy(predicts, Y_test)\n",
        "      loss_naive = CrossEntropy_Naive(predicts, Y_test)\n",
        "\n",
        "      return loss, loss_naive, predicts\n",
        "\n",
        "    def train(self, EPOCH, batch_size, alpha=0.5):\n",
        "        data = []\n",
        "        naive_model = get_naive_model()\n",
        "        # data_array, label = self.load_data()\n",
        "        data_array , label = gan_test_data, gan_test_labels\n",
        "        data, label = self.find_good_review(data_array, label)\n",
        "        print(np.array(data).shape)\n",
        "        niave_loss_1,loss_naive_1, pred_init = self.naive(naive_model, data, label)\n",
        "        print(\"***\")\n",
        "\n",
        "        new_data = []\n",
        "        for i in range(len(data)):\n",
        "          if pred_init[i] ==1 and label[i] ==1:\n",
        "            new_data.append(data[i])\n",
        "          if pred_init[i] ==0 and label[i] ==1:\n",
        "            print(\"wrong prediction\")\n",
        "\n",
        "        data = np.array(new_data)\n",
        "        encoded = get_embedding(data)\n",
        "\n",
        "        # print(\"after data selection\")\n",
        "        # niave_loss_1,loss_naive_1, pred_init = self.naive(naive_model, data, np.array([1 for i in range((data.shape)[0])]))\n",
        "        print(niave_loss_1,loss_naive_1)\n",
        "         \n",
        "        print(\"SHAPE ENCODED\")\n",
        "        print(encoded.shape)\n",
        "        X_train = encoded\n",
        "        # X_train  = np.random.rand(10, 200)\n",
        "        # X_train = np.expand_dims(X_train, axis=2)\n",
        "        \n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        # niave_loss = 200\n",
        "        # self.total_loss = np.float32(niave_loss)\n",
        "\n",
        "        for epoch in range(EPOCH):\n",
        "          print('avvale epoch')\n",
        "          print(\"epoch:\",epoch)\n",
        "          idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "          imgs = X_train[idx]\n",
        "          print(\"SELECTED INDEX:\", idx)\n",
        "          original =  get_decoded(imgs)\n",
        "          loss_1,loss_naive_1, pred1 = self.naive(naive_model, original, np.array([1 for i in range((original.shape)[0])]))\n",
        "          print(\"loss_naive_not_noisy\",loss_naive_1 )\n",
        "          print(\"loss_modified_not_noisy\", loss_1)\n",
        "          # print(\"DATA NUM\", imgs)\n",
        "          checkpointer = self.create_model_checkpoint('/content/drive/MyDrive/MLprj/', 'GAN', epoch)\n",
        "          g_loss = self.combined.fit(imgs,valid,epochs=1, batch_size=batch_size, callbacks=[checkpointer])\n",
        "          gen_imgs = self.generator.predict(imgs)\n",
        "\n",
        "          gen_imgs = np.expand_dims(gen_imgs, axis=2)\n",
        "          # print(\"NOISE_NUM\", gen_imgs)\n",
        "          noisy_image = 6*gen_imgs + imgs\n",
        "          decoded_noise = get_decoded(noisy_image)\n",
        "          # print(\"NOISE\", decoded_noise)\n",
        "          # print(\"NOISY OUT NUM\", noisy_image)\n",
        "          \n",
        "          d_loss_real = self.discriminator.fit(imgs, valid, epochs=1, batch_size=batch_size)\n",
        "          d_loss_fake = self.discriminator.fit(noisy_image, fake, epochs=1, batch_size=batch_size, steps_per_epoch=10)\n",
        "\n",
        "          # d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "          # noisy_image = tf.squeeze(noisy_image, axis=2)\n",
        "\n",
        "          naive_input = get_decoded(noisy_image)\n",
        "          \n",
        "          # print(\"NOISY IMAGE\", naive_input)\n",
        "          # print(naive_input.shape)\n",
        "\n",
        "\n",
        "          loss_2, loss_naive_2, pred2 = self.naive(naive_model, naive_input, np.array([1 for i in range((naive_input.shape)[0])]))\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "          for i in range(len(pred1)): \n",
        "            if pred1[i] != pred2[i]: \n",
        "                  print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "                  print(\"foundddd!!!!!\",  pred1[i],  pred2[i]) \n",
        "                  print(\"Review:\", original[i])\n",
        "                  print(\"NOISY Review\",naive_input[i])\n",
        "                  self.founded_examples.append({\"original\": original[i], \"noisy\": naive_input[i], \"index\": i, \"allindex\": idx, \"pred\": [pred1[i],  pred2[i]]})\n",
        "                  print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "                  truncated_sentence_original = original[i]\n",
        "                  truncated_sentence_noisy = naive_input[i]\n",
        "                  index = 0\n",
        "                  for j in range(len(original[i])):\n",
        "                    if j >0 :\n",
        "                      if  original[i][j-1] == original[i][j] == '?' and original[i][j+1]!='?':\n",
        "                        truncated_sentence_original = original[i][j+1:]\n",
        "                        index  = j+1\n",
        "                        break\n",
        "                  truncated_sentence_noisy = naive_input[i][index:]\n",
        "                  # for k in range(len(naive_input[i])):\n",
        "                  #     if k >0 :\n",
        "                  #       if  original[k-1] == original[k] == '?' and original[k+1]!='?':\n",
        "                  #         truncated_sentence_original = original[j+1:]\n",
        "                  #         break\n",
        "                  print(\"TRUNCATED_ORG\", truncated_sentence_original)\n",
        "                  print(\"TRUNCATED_NOISY\", truncated_sentence_noisy)\n",
        "                  _,_, predt1 = self.naive(naive_model, truncated_sentence_original, np.array([1 for i in range((truncated_sentence_original.shape)[0])]))\n",
        "                  _,_, predt2 = self.naive(naive_model, truncated_sentence_noisy, np.array([0 for i in range((truncated_sentence_noisy.shape)[0])]))\n",
        "          \n",
        "          print(\"\\n\\n\")\n",
        "          print(\"loss_naive_noisy\",loss_naive_2 )\n",
        "          print(\"loss_modified_noisy\", loss_2)\n",
        "\n",
        "          # self.total_loss = np.float32(niave_loss_2)\n",
        "          self.total_loss = np.float32(loss_2)\n",
        "\n",
        "          # self.array = np.zeros( shape = (self.batch_size,1))\n",
        "          self.array[0] = self.total_loss\n",
        "          print(\"akhare epoch\")\n",
        "\n",
        "        #     print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "        # if epoch % sample_interval == 0:\n",
        "        #     self.sample_images(epoch)\n",
        "\n",
        "\n",
        "\n",
        "gan = GAN()\n",
        "gan.train(EPOCH=30, batch_size=gan.batch_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "(200, 200)\n",
            "SCORE 0.515\n",
            "***\n",
            "1422.9974904703686 1340.1044211225862\n",
            "SHAPE ENCODED\n",
            "(103, 200, 50)\n",
            "avvale epoch\n",
            "epoch: 0\n",
            "SELECTED INDEX: [35 63 94 87 87 12 39 79  6 50]\n",
            "bade decode\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 205.9490\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-1.h5\n",
            "10/10 [==============================] - 2s 181ms/sample - loss: 205.9490\n",
            "bade decode\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 34ms/sample - loss: 0.5950 - accuracy: 1.0000\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 1s 131ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.6805 - accuracy: 0.5500\n",
            "bade decode\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 1\n",
            "SELECTED INDEX: [20 70 67 73 43 22 47 12 44 31]\n",
            "bade decode\n",
            "SCORE 0.0\n",
            "loss_naive_not_noisy 138.15510557964274\n",
            "loss_modified_not_noisy -9.999994999180669e-06\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 209.4550\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-1.h5\n",
            "10/10 [==============================] - 1s 71ms/sample - loss: 209.4550\n",
            "bade decode\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 0.8419 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 1s 128ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.4188 - accuracy: 1.0000\n",
            "bade decode\n",
            "SCORE 1.0\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 0 1\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'original' 'beautiful' 'movie' 'the' 'acting' 'is' 'great' 'the' '?' '?'\n",
            " '?' 'are' 'superb' 'paris' 'at' 'its' 'best' 'but' 'then' 'the' 'real'\n",
            " 'paris' 'not' 'the' 'famous' '?' 'and' 'the' 'music' 'will' 'do' 'also'\n",
            " 'a' 'brilliant' 'very' '?' 'which' 'i' 'just' 'very' 'much' 'love' 'br'\n",
            " 'br' 'the' 'best' 'french' 'movie' \"here's\" 'seen' 'and' 'french'\n",
            " 'cinema' 'is' 'very' 'good']\n",
            "NOISY Review ['you' 'black' '?' '?' '?' 'you' 'you' '?' '?' 'you' 'america' '?' 'you'\n",
            " '?' 'fat' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'black' 'you' '?' 'you' '?' '?' 'fat' '?' '?' '?' '?' 'remember' '?'\n",
            " '?' '?' 'america' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' 'you'\n",
            " '?' '?' '?' 'you' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " 'you' 'black' 'fat' '?' '?' 'you' 'you' 'you' 'you' '?' '?' '?' '?' '?'\n",
            " 'black' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' 'you' 'america' 'you' '?' '?' 'you' 'you'\n",
            " '?' '?' 'you' '?' '?' '?' '?' 'fat' '?' 'you' '?' '?' '?' '?' 'you' 'you'\n",
            " '?' '?' 'you' '?' '?' 'black' '?' 'you' 'original' 'inspired' 'movie'\n",
            " 'one' 'acting' 'the' 'battle' 'battle' 'fat' '?' '?' 'are' 'excellent'\n",
            " 'paris' 'at' 'its' 'best' 'but' 'finally' 'the' 'real' 'paris' 'not'\n",
            " 'one' 'famous' 'black' 'and' 'normal' 'music' 'will' 'do' 'by' 'a'\n",
            " 'inspired' 'extremely' '?' 'is' 'i' 'just' 'though' 'much' 'inspired'\n",
            " 'br' 'br' 'one' 'best' 'french' 'movie' \"here's\" 'seen' 'and' 'french'\n",
            " 'cinema' 'is' 'very' 'success']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c338100b90fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-c338100b90fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, EPOCH, batch_size, alpha)\u001b[0m\n\u001b[1;32m    307\u001b[0m                   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                       \u001b[0;32mif\u001b[0m  \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'?'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                         \u001b[0mtruncated_sentence_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                         \u001b[0mindex\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5X-5EkrvQGg"
      },
      "source": [
        "review = \"'well' 'sorry' 'for' 'the' 'mistake' 'on' 'the' 'one' 'line' '?' 'runs' 'people' 'run' 'this' 'movie' 'is' 'an' 'horror' 'imagine' 'gary' 'you' 'in' 'person' 'low' 'budget' 'movie' 'led' 'an' 'incredibly' 'bad' '?' 'heres' 'that' 'a' 'nightmare' 'did' 'well' 'yes' 'the' 'is' '?' 'run' 'i' 'give' 'it' 'out' 'of'\"\n",
        "\n",
        "review = review.replace(\"'\", \"\")\n",
        "\n",
        "print(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M36n4NUXRHBP"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi9q71wERJ4Y"
      },
      "source": [
        "from lime import lime_text\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "\n",
        "filename = '/content/drive/MyDrive/MLprj/naive_model.sav'\n",
        "\n",
        "def get_naive_model():\n",
        "    # load the model from disk\n",
        "    loaded_model = pickle.load(open(filename, 'rb'))\n",
        "    return loaded_model\n",
        "\n",
        "clf = get_naive_model()\n",
        "c = make_pipeline(vectorizer, clf)\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=[ 'negative', 'positive'])\n",
        "\n",
        "for rows in gan.founded_examples: \n",
        "\n",
        "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "    original = rows[\"original\"]\n",
        "    noisy = rows[\"noisy\"]\n",
        "    exp = explainer.explain_instance(original , c.predict_proba, num_features=2000)\n",
        "    print('Document id: %d' % idx)\n",
        "    print('Probability =', c.predict_proba([data_test[idx]]))\n",
        "    print('True class: %s' % y_test[idx])\n",
        "    exp.as_list()\n",
        "    %matplotlib inline\n",
        "    fig = exp.as_pyplot_figure()\n",
        "\n",
        "    exp = explainer.explain_instance(noisy , c.predict_proba, num_features=2000)\n",
        "    print('Document id: %d' % idx)\n",
        "    print('Probability =', c.predict_proba([data_test[idx]]))\n",
        "    print('True class: %s' % y_test[idx])\n",
        "    exp.as_list()\n",
        "    %matplotlib inline\n",
        "    fig = exp.as_pyplot_figure()\n",
        "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejXRDpl4946b"
      },
      "source": [
        "# `Reconstruct`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z3mnUb01cxJ"
      },
      "source": [
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/MLprj/GAN_NEW.h5\", compile=False)\n",
        "# input  = np.random.rand(10, 300, 1)\n",
        "gan = GAN()\n",
        "data_array, label = gan.load_data()\n",
        "data = []\n",
        "for i in range(len(data_array)):\n",
        "  if label[i] ==1:\n",
        "    data.append(data_array[i])\n",
        "\n",
        "data = np.array(data)[10:12]\n",
        "encoded = get_embedding(data)\n",
        "input = encoded\n",
        "print(\"DATA\",data)\n",
        "\n",
        "total_loss = 200\n",
        "array = np.zeros( shape = (10,1))\n",
        "array[0] += total_loss\n",
        "\n",
        "def custom_loss():\n",
        "  def vae_loss(fake, validity):\n",
        "        ep = 0.000001\n",
        "        t_loss = (-1)*(fake * K.log(validity) + (1 - fake) * K.log(1 - validity)) + array\n",
        "        print(t_loss)\n",
        "        print(\"****\")\n",
        "        print(K.mean(t_loss))\n",
        "        print(type(K.mean(t_loss)))\n",
        "        print(\"finished\")\n",
        "        return K.mean(t_loss)\n",
        "  return vae_loss\n",
        "\n",
        "vae_loss = custom_loss()\n",
        "reconstructed_model.compile(optimizer='Adam',\n",
        "                            loss=custom_loss())\n",
        "\n",
        "print(reconstructed_model.summary())\n",
        "input_layer = reconstructed_model.layers[0]\n",
        "g_layer = reconstructed_model.get_layer(\"tf_op_layer_add_18\")\n",
        "out = K.function([input_layer.input],[g_layer.output])\n",
        "out = out([input])[0]\n",
        "# print(out)\n",
        "naive_input = get_decoded(out)\n",
        "print(\"DEOCED,\", naive_input)\n",
        "\n",
        "\n",
        "# a = reconstructed_model.predict(input)\n",
        "\n",
        "# print(a)\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIgukDkDA5WP"
      },
      "source": [
        "print(out.shape)\n",
        "# # a = tf.math.reduce_sum([1,1,1])\n",
        "\n",
        "# a = np.array([1,2,3])\n",
        "# c = np.array([4,5,3])\n",
        "# b = K.constant(a)\n",
        "# c = K.constant(c)\n",
        "# print(1-b)\n",
        "# l=np.log(b)\n",
        "# # ep = 0.00000110]\n",
        "# print(l)\n",
        "# # b + ep\n",
        "m = [9,10]\n",
        "\n",
        "arr = np.array([[9, 9, 9, 9, 9, 9], [9,9,9,9,9,10]])\n",
        "occurrences = np.count_nonzero(arr == 9)\n",
        "print(occurrences)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}