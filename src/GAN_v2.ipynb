{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ5ucqgr4DPf",
        "outputId": "79559db5-fe98-4473-bf98-011673e63dff"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkAjf4BNkzib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae6db87-e3fd-45c4-e488-9b80e8311d48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MlxH3fqxRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1658f8-84a0-4cb6-ae06-3dc40e46dcc5"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# from model import VAE\n",
        "import keras\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from keras import objectives, backend as K\n",
        "import pickle\n",
        "\n",
        "\n",
        "disable_eager_execution()\n",
        "max_length = 200\n",
        "\n",
        "\n",
        "def read_vocab(path):\n",
        "    output = open(path, 'rb')\n",
        "    vocab = pickle.load(output)\n",
        "    output.close()\n",
        "    return vocab\n",
        "glove = read_vocab(\"/content/drive/MyDrive/MLprj/test2000\")\n",
        "\n",
        "def get_latent(data):\n",
        "  print(\"inja\")\n",
        "  print(data.shape)\n",
        "  reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Copy of modell.h5\", compile=False)\n",
        "\n",
        "  print(\"model ro khoond\")\n",
        "  def custom_loss():\n",
        "    def vae_loss(x, x_decoded_mean):\n",
        "          x = K.flatten(x)\n",
        "          x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "          xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
        "          return xent_loss \n",
        "    return vae_loss\n",
        "\n",
        "  vae_loss = custom_loss()\n",
        "\n",
        "  reconstructed_model.compile(optimizer='Adam',\n",
        "                              loss=custom_loss(), \n",
        "                                  metrics=['accuracy'])\n",
        "  \n",
        "  input_layer = reconstructed_model.layers[0]\n",
        "  encoder = reconstructed_model.get_layer(\"dense_2\")\n",
        "  print(\"reconstruct\")\n",
        "  # decoder = reconstructed_model.get_layer(\"decoded_mean\")\n",
        "\n",
        "  get_encoder_output = K.function([input_layer.input],[encoder.output])\n",
        "  print(\"aa\")\n",
        "  get_encoder_output = get_encoder_output([data])[0]\n",
        "  print(\"bbb\")\n",
        "  # get_decoder_output = K.function([encoder.output],[decoder.output])\n",
        "  # get_decoder_output = get_decoder_output([data])[0]\n",
        "  print(\"SHAPE:\",get_encoder_output.shape)\n",
        "  return get_encoder_output\n",
        "\n",
        "words_list = []\n",
        "vectors = []\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "for i in range(3, 2000): \n",
        "    word = reverse_word_index.get(i-3, '?')\n",
        "    vectors.append(glove[word])\n",
        "    words_list.append(word)\n",
        "\n",
        "def print_closest_words(vec):\n",
        "    closest_dist = 10000000\n",
        "    closest_word = -100\n",
        "    counter = 0 \n",
        "    for v in vectors: \n",
        "        dist = torch.norm(v - vec)\n",
        "        if closest_dist >= dist: \n",
        "            closest_word = counter \n",
        "            closest_dist = dist\n",
        "\n",
        "        counter += 1\n",
        "    return words_list[closest_word]\n",
        "\n",
        "\n",
        "def get_decoded(data):\n",
        "  # print(\"Latent SENTENCE\", data[0][0:10])\n",
        "\n",
        "  all = []\n",
        "  for sentence in data:\n",
        "    count = 0\n",
        "    decoded = []\n",
        "    for word in sentence:\n",
        "\n",
        "      decoded.append(print_closest_words(word))\n",
        "      # count += 1\n",
        "    all.append(decoded)\n",
        "  \n",
        "  print(\"bade decode\")\n",
        "  return np.array(all)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-vhcj5WCew"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def save_vocab(vocab, path):\n",
        "    with open(path, 'w+') as f:     \n",
        "        for token, index in vocab.stoi.items():\n",
        "            f.write(f'{index}\\t{token}')\n",
        "\n",
        "def read_vocab(path):\n",
        "    vocab = dict()\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            index, token = line.split('\\t')\n",
        "            vocab[token] = int(index)\n",
        "    return vocab\n",
        "    \n",
        "def get_embedding(dataset):\n",
        "    # The first time you run this will download a ~823MB file\n",
        "          # embedding size = 100\n",
        "    encoded = []\n",
        "    # print(\"DATASET[0]:\", dataset[0])\n",
        "    for sentence in dataset:\n",
        "        encoded.append(np.array([np.array(glove[word]) for word in sentence]))\n",
        "    # print(\"ENCODED[0]\", encoded[0][0:10])\n",
        "    return np.array(encoded)\n",
        "    \n",
        "# get_embedding()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNjlWv8QFVr1",
        "outputId": "99e5d735-1c3e-4898-9d76-f83b6f1733f1"
      },
      "source": [
        "!pip3 install hickle"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hickle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/ee/7f442cb653c22f6f1d9de922919be58d81bc8a09ec4f6d886ce447683596/hickle-4.0.4-py3-none-any.whl (49kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3.0.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (2.10.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from hickle) (1.19.5)\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from hickle) (0.3.3)\n",
            "Installing collected packages: hickle\n",
            "Successfully installed hickle-4.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZbDfgxvXJx"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBl_Snh3tVx-",
        "outputId": "dd59b7ad-0ef5-4842-a1ad-d87498ae6b56"
      },
      "source": [
        "import hickle as hkl\n",
        "def load_dataset():\n",
        "  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 2000) \n",
        "  print(train_data.shape)\n",
        "  print(test_data.shape)\n",
        "  # gan_test_data = test_data[0:2000]\n",
        "  # gan_test_labels = test_labels[0:2000]\n",
        "  \n",
        "  array_hkl = hkl.load('/content/drive/MyDrive/MLprj/data_new2000.hkl')\n",
        "  X_train = array_hkl['xtest']\n",
        "  gan_test_labels = array_hkl['ytest']\n",
        "  word_index = imdb.get_word_index()\n",
        "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "  gan_test_data = []\n",
        "  print(X_train.shape)\n",
        "  for i  in  range(len(X_train)):\n",
        "    decoded_review = [reverse_word_index.get(i - 3, '?') for i in X_train[i]]\n",
        "    gan_test_data.append(decoded_review)\n",
        "\n",
        "  niave_total_data = np.concatenate((train_data, test_labels[10000:]), axis = 0)\n",
        "  naive_total_labels = np.concatenate((train_labels, test_labels[10000:]), axis = 0)\n",
        "\n",
        "  return gan_test_data, gan_test_labels, niave_total_data, naive_total_labels\n",
        "\n",
        "gan_test_data, gan_test_labels, niave_total_data, naive_total_labels = load_dataset()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "(25000,)\n",
            "(25000,)\n",
            "(10000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oQZZWjoS7Gk"
      },
      "source": [
        "# Niave Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afm6y_UZS9ND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdd57b4-d967-4681-cf5f-5ece695bce39"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "filename = '/content/drive/MyDrive/MLprj/naive_model.sav'\n",
        "\n",
        "def preprocess(data):\n",
        "\n",
        "      truncated_data = []  \n",
        "      MAX_LENGTH= 200\n",
        "      for row in data:\n",
        "          if type(row) == type([1,1]):\n",
        "              truncated_data.append(row[:MAX_LENGTH])\n",
        "      data = np.array(truncated_data)\n",
        "      # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      # print(data[0])\n",
        "\n",
        "\n",
        "      def vectorize_sequences(sequences, dimension=2000):\n",
        "          results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "          for i,sequence in enumerate(sequences):\n",
        "              results[i,sequence] = 1   \n",
        "                          # Sets specific indices of results[i] to 1s\n",
        "          return results\n",
        "\n",
        "      # Vectorize training Data\n",
        "      # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      # print(data[0])\n",
        "      \n",
        "      data = vectorize_sequences(data)\n",
        "      # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      # print(data[0])\n",
        "\n",
        "\n",
        "      # MAX_LENGTH= 2000\n",
        "      # data = pad_sequences(data, maxlen=MAX_LENGTH)\n",
        "      # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\", data.shape)\n",
        "      # print(data[0])\n",
        "      return data\n",
        "\n",
        "\n",
        "def train_naive():\n",
        "    X = preprocess(niave_total_data)\n",
        "    \n",
        "\n",
        "    y = naive_total_labels\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    print(\"\\n\")\n",
        "    kf = KFold(n_splits=10, random_state=None, shuffle = False) \n",
        "    # X_train , Y_train, X_test, Y_test = load_data_Naive()\n",
        "    # X = np.concatenate((X_train, X_test), axis=0)\n",
        "    # y = np.concatenate((Y_train, Y_test), axis=0)\n",
        "    # i = 0\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    best_acc = 0\n",
        "    for train_index, test_index in kf.split(X):\n",
        "          # print(\"Train:\", train_index, \"Validation:\",test_index)\n",
        "          x_train, x_test = X[train_index], X[test_index] \n",
        "          y_train, y_test = y[train_index], y[test_index]\n",
        "          bnb = BernoulliNB(binarize=0.0)\n",
        "          bnb.fit(x_train, y_train)\n",
        "          y_pred = bnb.predict(x_train)\n",
        "          train_acc = accuracy_score(y_train, y_pred)\n",
        "          print(\"accuracy:\", train_acc)\n",
        "          train_accs.append(train_acc)\n",
        "          test_acc = bnb.score(x_test, y_test)\n",
        "          print(\"SCORE\",test_acc)\n",
        "          test_accs.append(test_acc)\n",
        "          if train_acc > best_acc:\n",
        "            print(\"THIS IS the max acc\", train_acc)\n",
        "            best_acc = train_acc\n",
        "            pickle.dump(bnb, open(filename, 'wb'))\n",
        "          print(\"*************\\n\")\n",
        "    print(\"Average Train Accuracy:\", np.mean(np.array(train_accs)))\n",
        "    print(\"Average Test Score:\", np.mean(np.array(test_accs)))\n",
        "\n",
        "train_naive()\n",
        "\n",
        "\n",
        "def get_naive_model():\n",
        "    # load the model from disk\n",
        "    loaded_model = pickle.load(open(filename, 'rb'))\n",
        "    return loaded_model\n",
        "\n",
        "get_naive_model()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 2000)\n",
            "(40000,)\n",
            "\n",
            "\n",
            "accuracy: 0.8259555555555556\n",
            "SCORE 0.806\n",
            "THIS IS the max acc 0.8259555555555556\n",
            "*************\n",
            "\n",
            "accuracy: 0.8250222222222222\n",
            "SCORE 0.8252\n",
            "*************\n",
            "\n",
            "accuracy: 0.8262666666666667\n",
            "SCORE 0.8256\n",
            "THIS IS the max acc 0.8262666666666667\n",
            "*************\n",
            "\n",
            "accuracy: 0.8261333333333334\n",
            "SCORE 0.8136\n",
            "*************\n",
            "\n",
            "accuracy: 0.8264888888888889\n",
            "SCORE 0.8224\n",
            "THIS IS the max acc 0.8264888888888889\n",
            "*************\n",
            "\n",
            "accuracy: 0.8264888888888889\n",
            "SCORE 0.8212\n",
            "*************\n",
            "\n",
            "accuracy: 0.8270666666666666\n",
            "SCORE 0.8064\n",
            "THIS IS the max acc 0.8270666666666666\n",
            "*************\n",
            "\n",
            "accuracy: 0.8252\n",
            "SCORE 0.824\n",
            "*************\n",
            "\n",
            "accuracy: 0.826\n",
            "SCORE 0.8196\n",
            "*************\n",
            "\n",
            "accuracy: 0.8255555555555556\n",
            "SCORE 0.8204\n",
            "*************\n",
            "\n",
            "Average Train Accuracy: 0.8260177777777779\n",
            "Average Test Score: 0.8184400000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkC8DUp1TCwY"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYCRPTGQCVn3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3627a73-ade7-4e90-ce27-ce188f2e6c96"
      },
      "source": [
        "import os\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization, LSTM, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import hickle as hkl\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.vector_size = 200\n",
        "        self.txt_shape = (self.vector_size,50,)\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "        self.batch_size = 10\n",
        "        self.generator = self.build_generator()\n",
        "        self.total_loss = 200\n",
        "        self.array = np.zeros( shape = (self.batch_size,1))\n",
        "        self.array[0] += self.total_loss\n",
        "\n",
        "        # z = Input(shape=(self.latent_dim,))\n",
        "        data = Input(shape=self.txt_shape)\n",
        "        noise = self.generator(data)\n",
        "        noise = tf.expand_dims(noise, axis=2)\n",
        "        noisy_img = data + noise\n",
        "        # print(noisy_img.shape)\n",
        "        # noisy_image = K.sum(noise, data)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "        validity = self.discriminator(noisy_img)\n",
        "        self.combined = Model(data, validity)\n",
        "        self.combined.compile(loss= self.get_total_loss, optimizer=optimizer)\n",
        "\n",
        "        self.founded_examples = []\n",
        "  \n",
        "    def get_total_loss(self, fake, validity):\n",
        "      ep = 0.000001\n",
        "      t_loss = 3*(K.sum((-1)*(fake * K.log(validity) + (1 - fake) * K.log(1 - validity)))) + self.array[0]\n",
        "\n",
        "      return t_loss\n",
        "\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(50))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.vector_size, activation='tanh'))\n",
        "        # model.summary()\n",
        "        data = Input(shape = self.txt_shape)\n",
        "        noise = model(data)\n",
        "\n",
        "        return Model(data, noise)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        LSTM_units = 128\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(LSTM_units))\n",
        "        model.add(Dense(100))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(10))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # model.summary()\n",
        "        img = Input(shape = self.txt_shape)\n",
        "        validity = model(img)\n",
        "        return Model(img, validity)\n",
        "        \n",
        "\n",
        "    def create_model_checkpoint(self, dir, model_name, epoch):\n",
        "      filepath = dir + '/' + \\\n",
        "                model_name + \"-{epoch:%d}.h5\"%epoch\n",
        "      directory = os.path.dirname(filepath)\n",
        "\n",
        "      try:\n",
        "          os.stat(directory)\n",
        "      except:\n",
        "          os.mkdir(directory)\n",
        "\n",
        "      checkpointer = ModelCheckpoint(filepath=filepath,\n",
        "                                    verbose=1,\n",
        "                                    save_best_only=False)\n",
        "\n",
        "      return checkpointer\n",
        "\n",
        "    def save_decoded_original(self,data, path):\n",
        "        output = open(path, 'wb')\n",
        "        pickle.dump(data, output)\n",
        "        output.close()\n",
        "\n",
        "    def load_decoded_original(self, path):\n",
        "        output = open(path, 'rb')\n",
        "        data = pickle.load(output)\n",
        "        output.close()\n",
        "        return data  \n",
        "\n",
        "    def find_good_review(self, data_array, label):\n",
        "        find = True\n",
        "        number = 200\n",
        "        if find: \n",
        "            newdata = []\n",
        "            newlabel = []\n",
        "            for i in range(len(data_array)):\n",
        "                sentence_started = False\n",
        "                count = 0\n",
        "                for word in data_array[i]:\n",
        "                    if word != \"?\" and (not sentence_started):\n",
        "                          sentence_started = True\n",
        "                    if sentence_started and word == \"?\":\n",
        "                          count += 1\n",
        "                    if count >6: \n",
        "                          break \n",
        "                if count <6: \n",
        "                    newdata.append(data_array[i])\n",
        "                    newlabel.append(label[i])\n",
        "                    number -= 1\n",
        "                if number <=0: \n",
        "                    return  newdata, np.array(newlabel)\n",
        "\n",
        "            print(\"Only found: \", 200 -number)\n",
        "            return  newdata, np.array(newlabel)\n",
        "\n",
        "        return np.array(data_array)[0:number], np.array(label)[0:number]\n",
        "\n",
        "    def pre_process(self, X_test):\n",
        "      new_test = []\n",
        "      tmp = []\n",
        "      for sentence in X_test:\n",
        "        for word in sentence:\n",
        "          if word == '?':\n",
        "            tmp.append(0)\n",
        "          else:\n",
        "            tmp.append(word_index[word])\n",
        "        new_test.append(list(tmp))\n",
        "      x_test = np.array(new_test)\n",
        "      \n",
        "      def vectorize_sequences(sequences, dimension=2000):\n",
        "          results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "          for i,sequence in enumerate(sequences):\n",
        "              results[i,sequence] = 1                        # Sets specific indices of results[i] to 1s\n",
        "          return results\n",
        "\n",
        "      MAX_LENGTH= 200\n",
        "      # Vectorize testing Data\n",
        "\n",
        "      truncated_data = []  \n",
        "\n",
        "      for row in x_test:\n",
        "          if type(row) == type([1,1]):\n",
        "              truncated_data.append(row[:MAX_LENGTH])\n",
        "      x_test = np.array(truncated_data)\n",
        "\n",
        "      X_test = vectorize_sequences(x_test)\n",
        "      # X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
        "     \n",
        "      return X_test\n",
        "\n",
        "    def truncate_noisy_data(self, noisy, original):\n",
        "      truncated_sentence_original= \"\"\n",
        "      index = 0\n",
        "      indx_list = []\n",
        "      for i in range(len(original)):\n",
        "          for j in range(len(original[i])):\n",
        "              if j >0 :\n",
        "                  if  original[i][j-1] == original[i][j] == '?' and original[i][j+1]!='?':\n",
        "                    truncated_sentence_original = original[i][j+1:]\n",
        "                    index  = j+1\n",
        "                    indx_list.append(index)\n",
        "                    break\n",
        "          \n",
        "\n",
        "          part1 = ['?' for i in range(index)] \n",
        "          part2 = [item for item in noisy[i][index:]]\n",
        "          noisy[i] = part1 + part2\n",
        "\n",
        "  \n",
        "          # print(\"TRUNCATED_ORG\", truncated_sentence_original)\n",
        "\n",
        "      return noisy, indx_list\n",
        "\n",
        "\n",
        "    def naive(self, bnb, X_test, Y_test):\n",
        "  \n",
        "      X_test = self.pre_process(X_test)\n",
        "      print(\"SCORE\",bnb.score(X_test, Y_test))\n",
        "      \n",
        "      def CrossEntropy(pred, y):\n",
        "\n",
        "              ep = 0.000001\n",
        "              pos = np.sum(y * -np.log(1 - pred + ep))\n",
        "              neg = np.sum((1-y) * -np.log(pred + ep))\n",
        "              # print(type(pos))\n",
        "              neg + pos\n",
        "            \n",
        "              return neg + pos\n",
        "\n",
        "\n",
        "      def CrossEntropy_Naive(pred, y):\n",
        "        ep = 0.000001\n",
        "        pos = np.sum(y * -np.log(pred + ep))\n",
        "        neg = np.sum((1-y) * -np.log(1-pred + ep))\n",
        "       \n",
        "        neg + pos\n",
        "        return neg + pos\n",
        "\n",
        "      predicts = bnb.predict(X_test)\n",
        "      loss = CrossEntropy(predicts, Y_test)\n",
        "      loss_naive = CrossEntropy_Naive(predicts, Y_test)\n",
        "\n",
        "      return loss, loss_naive, predicts\n",
        "\n",
        "    def train(self, EPOCH, batch_size, alpha=0.5):\n",
        "        data = []\n",
        "        naive_model = get_naive_model()\n",
        "        # data_array, label = self.load_data()\n",
        "        data_array , label = gan_test_data, gan_test_labels\n",
        "        data, label = self.find_good_review(data_array, label)\n",
        "        niave_loss_1,loss_naive_1, pred_init = self.naive(naive_model, data, label)\n",
        "        print(\"***\")\n",
        "\n",
        "        new_data = []\n",
        "        for i in range(len(data)):\n",
        "          if pred_init[i] ==1 and label[i] ==1:\n",
        "            new_data.append(data[i])\n",
        "          if pred_init[i] ==0 and label[i] ==1:\n",
        "            print(\"wrong prediction\")\n",
        "\n",
        "        data = np.array(new_data)\n",
        "\n",
        "        encoded = get_embedding(data)\n",
        "        # all_original = get_decoded(encoded)\n",
        "        # print(\"saving original\")\n",
        "        # self.save_decoded_original(all_original, \"/content/drive/MyDrive/MLprj/encoded_original.hkl\")\n",
        "        # print(\"reading original\")\n",
        "        all_original = self.load_decoded_original(\"/content/drive/MyDrive/MLprj/encoded_original.hkl\")\n",
        "        all_original = np.array(all_original)\n",
        "        print(\"done with original\")\n",
        "\n",
        "        print(niave_loss_1,loss_naive_1)\n",
        "         \n",
        "        print(\"SHAPE ENCODED\")\n",
        "        print(encoded.shape)\n",
        "        X_train = encoded\n",
        "        \n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(EPOCH):\n",
        "          print('avvale epoch')\n",
        "          print(\"epoch:\",epoch)\n",
        "          idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "\n",
        "\n",
        "          imgs = X_train[idx]\n",
        "          print(\"SELECTED INDEX:\", idx)\n",
        "          # original =  get_decoded(imgs)\n",
        "          original = all_original[idx] \n",
        "          loss_1,loss_naive_1, pred1 = self.naive(naive_model, original, np.array([1 for i in range((original.shape)[0])]))\n",
        "          print(\"loss_naive_not_noisy\",loss_naive_1 )\n",
        "          print(\"loss_modified_not_noisy\", loss_1)\n",
        "\n",
        "          checkpointer = self.create_model_checkpoint('/content/drive/MyDrive/MLprj/', 'GAN', epoch)\n",
        "          g_loss = self.combined.fit(imgs,valid,epochs=1, batch_size=batch_size, callbacks=[checkpointer])\n",
        "          gen_imgs = self.generator.predict(imgs)\n",
        "\n",
        "          gen_imgs = np.expand_dims(gen_imgs, axis=2)\n",
        "\n",
        "          noisy_image = 6*gen_imgs + imgs\n",
        "\n",
        "          decoded_noise = get_decoded(noisy_image)\n",
        "          print(decoded_noise[0])\n",
        "          print(\"test\")\n",
        "          # decoded_noise, index_list = self.truncate_noisy_data(decoded_noise,  original)\n",
        "\n",
        "          # print(\"NOISE\", decoded_noise)\n",
        "          # print(\"NOISY OUT NUM\", noisy_image)\n",
        "\n",
        "          # print(\"truncated!!!!!!\")\n",
        "\n",
        "          # for i in range(len(index_list)): \n",
        "          #       noisy_image[i] = list(imgs[i][:index_list[i]]) + list(noisy_image[i][index_list[i]:])\n",
        "          # noisy_image = np.array(noisy_image)\n",
        "\n",
        "          d_loss_real = self.discriminator.fit(imgs, valid, epochs=1, batch_size=batch_size)\n",
        "          d_loss_fake = self.discriminator.fit(noisy_image, fake, epochs=1, batch_size=batch_size, steps_per_epoch=10)\n",
        "\n",
        "          # d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "          # noisy_image = tf.squeeze(noisy_image, axis=2)\n",
        "\n",
        "\n",
        "          naive_input = decoded_noise\n",
        "          loss_2, loss_naive_2, pred2 = self.naive(naive_model, naive_input, np.array([1 for i in range((naive_input.shape)[0])]))\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "          for i in range(len(pred1)): \n",
        "            if pred1[i] != pred2[i]: \n",
        "                  print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "                  print(\"foundddd!!!!!\",  pred1[i],  pred2[i]) \n",
        "                  print(\"Review:\", original[i])\n",
        "                  print(\"NOISY Review\",naive_input[i])\n",
        "                  self.founded_examples.append({\"original\": original[i], \"noisy\": naive_input[i], \"index\": i, \"allindex\": idx, \"pred\": [pred1[i],  pred2[i]]})\n",
        "                  print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "          print(\"\\n\\n\")\n",
        "          print(\"loss_naive_noisy\",loss_naive_2 )\n",
        "          print(\"loss_modified_noisy\", loss_2)\n",
        "\n",
        "          # self.total_loss = np.float32(niave_loss_2)\n",
        "          self.total_loss = np.float32(loss_2)\n",
        "\n",
        "          # self.array = np.zeros( shape = (self.batch_size,1))\n",
        "          self.array[0] = self.total_loss\n",
        "          print(\"akhare epoch\")\n",
        "\n",
        "        #     print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "        # if epoch % sample_interval == 0:\n",
        "        #     self.sample_images(epoch)\n",
        "\n",
        "\n",
        "\n",
        "gan = GAN()\n",
        "gan.train(EPOCH=30, batch_size=gan.batch_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE 0.515\n",
            "***\n",
            "trying to decode\n",
            "done with original\n",
            "1422.9974904703686 1340.1044211225862\n",
            "SHAPE ENCODED\n",
            "(103, 200, 50)\n",
            "avvale epoch\n",
            "epoch: 0\n",
            "SELECTED INDEX: [23 27 47 10 32 15 34 77 45 98]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 221.6497\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-1.h5\n",
            "10/10 [==============================] - 2s 164ms/sample - loss: 221.6497\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you'\n",
            " 'remember' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " 'america' '?' 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' 'black' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' 'you' '?' 'you' '?' '?' '?' 'you' 'you' '?' 'you' '?' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'america' '?' 'you' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' 'black' 'you' 'you' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'america' '?' 'you' 'in' 'your'\n",
            " 'opinion' 'this' 'is' 'the' 'best' '?' 'bed' 'flick' 'probably' 'more'\n",
            " 'because' 'of' '?' 'than' 'sure' 'never' '?' 'remember' 'full' 'of' '?'\n",
            " 'from' 'the' 'first' 'moment' 'can' 'its' 'dark' 'ended']\n",
            "test\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 44ms/sample - loss: 0.7226 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 1s 147ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.6099 - accuracy: 0.8600\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 1\n",
            "SELECTED INDEX: [ 23  63   6  15  18  93  40  97 100  47]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 227.1514\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-1.h5\n",
            "10/10 [==============================] - 1s 58ms/sample - loss: 227.1514\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'america' '?'\n",
            " 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' 'black' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?'\n",
            " 'you' '?' '?' '?' 'you' 'you' '?' 'you' '?' '?' '?' '?' '?' 'you' '?'\n",
            " 'you' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'america' '?'\n",
            " 'you' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' 'black' 'you' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' 'america' '?' 'you' 'in' 'your' 'opinion' 'this'\n",
            " 'is' 'the' 'best' '?' 'bed' 'flick' 'probably' 'more' 'because' 'of' '?'\n",
            " 'than' 'sure' 'never' '?' 'remember' 'full' 'of' '?' 'from' 'the' 'first'\n",
            " 'moment' 'can' 'its' 'dark' 'ended']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 0.8985 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 1s 140ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.4475 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 2\n",
            "SELECTED INDEX: [ 70  31  92 102  97   4  15  32  83  81]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 234.7320\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN- 1.h5\n",
            "10/10 [==============================] - 1s 62ms/sample - loss: 234.7320\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?' '?'\n",
            " 'america' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?'\n",
            " 'remember' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'remember' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'this' 'soap' 'was' 'a' '?' 'led' 'me' 'while' 'i' 'was' '?' 'from'\n",
            " 'channel' 'to' 'channel' 'i' \"here's\" 'know' 'know' 'but' 'can' 'filled'\n",
            " 'in' 'me' 'with' '?' 'and' '?' 'this' 'is' 'how' 'a' 'high' 'budget'\n",
            " 'movie' 'can' 'did' 'do' 'mostly' 'i' 'liked' 'it' 'this' 'is' 'a' 'must'\n",
            " 'see' 'one' 'br' 'br']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 1.1420 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 153ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.3215 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 3\n",
            "SELECTED INDEX: [52 58 80 52 32 40 68 10 28 70]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 250.4919\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-  1.h5\n",
            "10/10 [==============================] - 1s 60ms/sample - loss: 250.4919\n",
            "bade decode\n",
            "['?' 'remember' 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' 'you' '?' '?'\n",
            " '?' '?' 'black' 'you' 'you' '?' '?' '?' 'you' '?' 'you' 'you' 'america'\n",
            " '?' '?' '?' '?' 'you' 'america' 'black' 'remember' '?' '?' 'you' '?'\n",
            " 'remember' '?' 'you' '?' '?' '?' '?' '?' 'remember' 'you' 'you' '?' '?'\n",
            " '?' 'you' '?' 'you' '?' 'you' 'black' '?' '?' '?' '?' '?' 'remember' '?'\n",
            " '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' 'you'\n",
            " '?' '?' '?' '?' 'you' 'you' '?' 'you' '?' '?' '?' 'you' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'this' 'the' 'one'\n",
            " 'worth' 'watched' 'although' 'it' 'is' 'sometimes' 'cheesy' 'it' 'is'\n",
            " 'great' 'to' 'can' 'a' 'young' '?' 'black' 'and' 'situations' 'ends' 'up'\n",
            " 'being' 'quite' 'the' 'entertaining' 'and' 'humorous' 'action' 'movie'\n",
            " 'i' 'watched' 'it' 'many' 'times' 'when' 'never' 'person' 'young' 'and'\n",
            " 'the' 'still' 'enjoy' 'it' 'gets' 'i' 'pop' 'the' 'old' 'vhs' 'into'\n",
            " 'one' 'machine' 'i' 'happen' 'to' 'battle' 'a' 'copy' 'so' 'bed' 'back'\n",
            " 'with' 'the' 'movie' 'let' 'reality' 'eat' 'for' 'a' 'little' 'while'\n",
            " 'and' 'you' 'will' 'be' 'able' 'to' 'have' 'a' 'few' 'good' 'laughs'\n",
            " 'and' 'an' 'enjoyable' 'hour' 'led' 'a' 'half']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 17ms/sample - loss: 1.6703 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 151ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.2156 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 4\n",
            "SELECTED INDEX: [ 6 99 82 57 95 54 56 75 65 79]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 264.5773\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-   1.h5\n",
            "10/10 [==============================] - 1s 58ms/sample - loss: 264.5773\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' 'you' 'you' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'black' '?' '?' '?' '?'\n",
            " 'you' '?' 'battle' '?' '?' 'america' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'america' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' 'you' '?' 'you' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' 'remember' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' '?' 'you' '?' '?' 'you' '?' 'you' 'you' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' 'you' '?' 'you' 'black' '?' 'america' '?' '?'\n",
            " '?' 'remember' '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'you' 'not' 'many'\n",
            " 'people' 'are' 'seen' 'this' 'film' 'and' 'it' 'is' 'a' 'shame' 'can'\n",
            " 'it' 'is' 'a' 'work' 'battle' 'art' 'the' 'characters' 'can' 'brilliant'\n",
            " 'the' 'dialogue' 'is' '?' 'well' 'normal' 'use' 'of' 'themes' 'leaves'\n",
            " 'the' 'audience' 'wondering' 'i' 'truly' 'loved' 'this' 'film' 'and'\n",
            " \"here's\" 'decided' 'the' 'see' 'some' 'of' '?' '?' 'situations']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.1610 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 1s 148ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1477 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 5\n",
            "SELECTED INDEX: [96 42 97  1 37 99 49 47 75 94]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 271.4680\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-    1.h5\n",
            "10/10 [==============================] - 1s 80ms/sample - loss: 271.4680\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' 'you' 'you' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' '?' 'remember' '?' 'you' 'black' '?' '?'\n",
            " '?' '?' 'you' '?' 'battle' '?' '?' 'america' 'you' '?' 'america' '?' '?'\n",
            " '?' '?' 'you' '?' '?' 'black' '?' '?' 'remember' '?' '?' 'you' 'you'\n",
            " 'you' 'you' 'you' '?' 'america' '?' '?' 'fat' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'remember' '?' '?'\n",
            " 'you' '?' 'remember' 'you' 'you' '?' 'you' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' 'black' '?' '?' 'america' 'you' '?' '?' 'black' 'you' '?' 'you' '?'\n",
            " '?' 'you' '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " 'fat' '?' 'you' 'black' '?' 'black' '?' '?' '?' 'america' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' 'you' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?'\n",
            " '?' 'you' 'you' 'you' '?' '?' '?' 'america' '?' '?' '?' 'can' 'to' 'some'\n",
            " 'people' 'look' 'a' 'bit' 'normal' 'but' \"here's\" 'the' 'of' 'rare'\n",
            " 'battle' 'adventure' 'film' 'the' 'could' 'be' 'compare' 'to' 'with'\n",
            " 'the' 'bit' 'of' '?' 'you' 'must' 'see' 'can']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.3673 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 153ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1011 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 6\n",
            "SELECTED INDEX: [82 32 54 40 25 59 67 11 66 39]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.7671\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-     1.h5\n",
            "10/10 [==============================] - 1s 65ms/sample - loss: 272.7671\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'remember' '?' '?' '?' '?' 'you'\n",
            " '?' 'battle' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' 'america' '?' '?' 'i' 'originally' 'saw' 'the' 'film'\n",
            " 'while' 'i' 'was' 'working' 'as' 'a' '?' 'sure' 'musical' 'theatre' 'in'\n",
            " 'summer' '?' 'if' \"here's\" 'ever' 'done' 'any' 'work' 'in' 'theatre'\n",
            " 'especially' 'at' 'a' 'summer' '?' 'theatre' \"here's\" 'really' 'enjoy'\n",
            " 'can' 'film' 'br' 'br' 'yes' 'there' 'are' 'more' 'moments' 'of' 'really'\n",
            " 'bad' 'writing' 'in' 'the' 'film' 'but' 'overall' \"here's\" 'a' 'lovely'\n",
            " '?' 'to' 'the' 'theatre' 'and' 'why' 'people' 'love' 'can']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.3747 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 152ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0898 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'originally' 'saw' 'this' 'film'\n",
            " 'while' 'i' 'was' 'working' 'as' 'a' '?' 'doing' 'musical' 'theatre' 'in'\n",
            " 'summer' '?' 'if' \"here's\" 'ever' 'done' 'any' 'work' 'in' 'theatre'\n",
            " 'especially' 'at' 'a' 'summer' '?' 'theatre' \"here's\" 'really' 'enjoy'\n",
            " 'this' 'film' 'br' 'br' 'yes' 'there' 'are' 'some' 'moments' 'of'\n",
            " 'really' 'bad' 'writing' 'in' 'the' 'film' 'but' 'overall' \"here's\" 'a'\n",
            " 'lovely' '?' 'to' 'the' 'theatre' 'and' 'why' 'people' 'love' 'it']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'remember' '?' '?' '?' '?' 'you'\n",
            " '?' 'battle' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' 'america' '?' '?' 'i' 'originally' 'saw' 'the' 'film'\n",
            " 'while' 'i' 'was' 'working' 'as' 'a' '?' 'sure' 'musical' 'theatre' 'in'\n",
            " 'summer' '?' 'if' \"here's\" 'ever' 'done' 'any' 'work' 'in' 'theatre'\n",
            " 'especially' 'at' 'a' 'summer' '?' 'theatre' \"here's\" 'really' 'enjoy'\n",
            " 'can' 'film' 'br' 'br' 'yes' 'there' 'are' 'more' 'moments' 'of' 'really'\n",
            " 'bad' 'writing' 'in' 'the' 'film' 'but' 'overall' \"here's\" 'a' 'lovely'\n",
            " '?' 'to' 'the' 'theatre' 'and' 'why' 'people' 'love' 'can']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'this' 'is' 'a' 'classic' 'that' 'will' 'be' 'able'\n",
            " 'to' 'hold' 'up' 'with' '?' 'to' 'come' 'simply' 'because' 'of' 'the'\n",
            " 'fact' 'that' 'it' 'is' 'shot' 'with' 'a' \"here's\" 'style' 'and' \"here's\"\n",
            " 'a' 'story' 'about' 'the' \"here's\" 'it' 'is' 'funny' 'action' 'filled'\n",
            " 'entertaining' 'and' 'sad' 'at' 'the' 'same' 'time' 'it' 'has' 'the'\n",
            " 'effect' 'to' 'pull' 'you' 'into' 'the' 'lives' 'of' 'these' 'poor'\n",
            " 'folks' 'and' 'the' '?' 'for' 'their' 'actions' '4' 'stars']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'remember' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' 'black' '?' '?' '?' '?' 'you' 'remember' '?' '?' '?' '?' 'you'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' 'you'\n",
            " '?' 'this' 'part' 'a' 'classic' 'that' 'will' 'be' 'able' 'to' 'hold'\n",
            " 'up' 'with' '?' 'to' 'come' 'simply' 'because' 'of' 'the' 'fact' 'that'\n",
            " 'it' 'is' 'shot' 'with' 'a' \"here's\" 'style' 'and' \"here's\" 'a' 'story'\n",
            " 'about' 'the' \"here's\" 'it' 'is' 'funny' 'action' 'filled' 'entertaining'\n",
            " 'and' 'sad' 'at' 'the' 'same' 'time' 'it' 'has' 'the' 'effect' 'to'\n",
            " 'pull' 'you' 'into' 'the' 'lives' 'of' 'these' 'poor' 'folks' 'and' 'the'\n",
            " '?' 'for' 'their' 'actions' '4' 'stars']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' 'amazing' 'movie' 'some' 'of' 'the' 'script'\n",
            " 'writing' 'could' 'have' 'been' 'better' 'some' '?' 'language' '?' 'the'\n",
            " 'dead' 'is' '?' 'to' 'throughout' 'the' 'movie' 'beautiful' 'scenery'\n",
            " 'and' 'great' 'acting' 'very' '?' 'highly' 'recommend']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?' '?' '?' '?' 'you' '?'\n",
            " 'battle' '?' '?' '?' '?' '?' 'remember' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' 'you' 'you' 'you' '?' '?' 'remember' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' 'america' '?' '?' 'remember' 'you' '?' '?'\n",
            " 'remember' '?' '?' '?' '?' '?' 'you' '?' 'you' 'you' '?' '?' 'you' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' 'you' '?' 'you' 'black' '?' '?' '?' '?' '?'\n",
            " 'remember' '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' 'you' 'amazing'\n",
            " 'movie' 'more' 'of' 'the' 'script' 'writing' 'could' 'have' 'be' 'better'\n",
            " 'some' '?' 'language' '?' 'the' 'dead' 'is' '?' 'to' 'throughout' 'the'\n",
            " 'movie' 'beautiful' 'scenery' 'and' 'great' 'acting' 'very' '?' 'highly'\n",
            " 'recommend']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'br' 'br' 'is' 'simply' 'david' '?' 'best' 'movie' 'all'\n",
            " 'the' 'people' 'compare' 'it' 'to' 'the' '?' \"here's\" 'not' 'even'\n",
            " 'similar' 'if' 'you' 'enjoyed' '?' 'other' 'works' 'just' 'a' 'little'\n",
            " 'bit' \"here's\" 'love' 'this' 'one']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'remember' '?' 'you' '?' '?' '?' 'you' '?' '?' 'you' 'america' '?' '?'\n",
            " '?' '?' 'you' '?' 'black' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' 'you' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you'\n",
            " '?' '?' '?' '?' 'br' 'br' 'is' 'simply' 'david' '?' 'best' 'hollywood'\n",
            " 'all' 'the' 'people' 'compare' 'it' 'to' 'the' '?' \"here's\" 'not' 'even'\n",
            " 'similar' 'if' 'you' 'enjoyed' '?' 'other' 'works' 'came' 'a' 'little'\n",
            " 'bit' \"here's\" 'love' 'this' 'person']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'first' 'saw'\n",
            " 'this' 'movie' 'in' 'the' 'night' '?' 'of' 'one' 'of' 'my' 'favourite'\n",
            " 'tv' '?' 'i' 'was' '?' 'from' 'the' 'very' 'first' 'minute' 'nothing'\n",
            " 'is' 'as' 'it' 'first' 'seems' 'lots' 'of' 'suspense' 'great' 'acting'\n",
            " 'from' 'mr' 'van' '?' 'and' 'i' 'did' 'not' 'mind' 'the' '?' 'in' 'it'\n",
            " 'one' 'bit' 'and' 'best' 'of' 'all' 'you' 'are' 'in' 'for' 'a' 'surprise'\n",
            " 'ending']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'black' 'you' 'you' '?' '?' '?' 'you' '?' 'you' 'you' 'america' '?' '?'\n",
            " '?' '?' 'you' '?' 'black' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you'\n",
            " 'you' '?' '?' 'you' 'you' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you' 'you'\n",
            " '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' 'america' 'i'\n",
            " 'first' 'saw' 'this' 'movie' 'in' 'the' 'night' 'you' 'natural' 'one'\n",
            " 'of' 'my' 'favourite' 'television' 'you' 'i' 'was' '?' 'from' 'the'\n",
            " 'very' 'one' 'minute' 'nothing' 'is' 'as' 'the' 'first' 'seems' 'lots'\n",
            " 'natural' 'suspense' 'great' 'acted' 'from' 'mr' 'van' 'you' 'and' 'i'\n",
            " 'did' 'not' 'mind' 'the' '?' 'normal' 'it' 'one' 'bit' 'and' 'best' 'of'\n",
            " 'all' 'you' 'are' 'in' 'for' 'a' 'surprise' 'normal']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'this' 'movie' 'was' 'thought' 'to'\n",
            " 'be' 'low' 'budget' 'but' 'it' 'turned' 'out' 'to' 'be' 'awesome' 'i'\n",
            " 'just' 'rented' 'it' 'from' '?' 'and' 'i' 'loved' 'it' 'the' 'acting'\n",
            " 'was' 'very' 'good' 'hot' 'women' 'and' 'some' 'scary' 'parts' 'it' 'is'\n",
            " 'plain' 'and' 'simply' 'worth' 'the' 'money' 'to' 'pay' 'for']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' 'you' 'you' '?' '?' '?' 'you' '?' '?' 'you' 'america' '?' '?'\n",
            " '?' '?' 'you' '?' 'black' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' 'america' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' 'you'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' 'you' '?' '?' '?' '?' 'this' 'movie' 'was' 'thought' 'to' 'be'\n",
            " 'low' 'budget' 'can' 'it' 'turned' 'out' 'to' 'although' 'awesome' 'i'\n",
            " 'just' 'bed' 'it' 'from' '?' 'and' 'i' 'loved' 'can' 'the' 'acting' 'was'\n",
            " 'very' 'good' 'hot' 'women' 'and' 'some' 'scary' 'parts' 'it' 'is'\n",
            " 'plain' 'and' 'simply' 'worth' 'the' 'money' 'to' 'pay' 'needs']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'the' 'best' 'movie' 'about' 'friendship' 'especially' 'between' 'an'\n",
            " '?' '?' 'person' 'and' 'a' 'normal' 'person' 'this' 'is' 'a' 'great'\n",
            " 'movie' 'for' 'everyone' 'to' 'see' 'even' 'though' 'there' 'is' 'strong'\n",
            " 'language' 'used' 'i' 'have' 'seen' 'it' '?' 'times']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' 'you' '?' '?' 'you' 'america' '?' '?' '?' '?' 'you'\n",
            " '?' 'battle' '?' '?' '?' '?' 'you' 'america' '?' '?' '?' '?' 'you' '?'\n",
            " '?' 'america' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' 'you' 'you' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' 'you' '?' '?'\n",
            " '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' 'you' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' 'the' 'best' 'movie' 'ago' 'friendship' 'especially'\n",
            " 'between' 'an' '?' '?' 'person' 'and' 'a' 'normal' 'person' 'this' 'is'\n",
            " 'a' 'great' 'movie' 'the' 'everyone' 'to' 'see' 'even' 'though' 'there'\n",
            " 'is' 'strong' 'language' 'used' 'i' 'have' 'seen' 'it' '?' 'situations']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' 'what' 'can' 'i' 'say' 'an' 'excellent' 'end'\n",
            " 'to' 'an' 'excellent' 'series' 'it' 'never' 'quite' 'got' 'the' '?' 'it'\n",
            " 'deserved' 'in' '?' 'but' 'by' 'far' 'the' 'best' 'cop' 'show' 'with'\n",
            " 'the' 'best' 'writing' 'and' 'the' 'best' 'cast' 'on' '?' 'ever' 'the'\n",
            " 'end' 'of' 'a' 'great' 'era' 'sorry' 'to' 'see' 'you' 'go']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' 'you' 'you' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'black' '?' '?' '?' '?' 'you'\n",
            " '?' 'battle' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' 'remember' '?' '?' 'remember' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' 'you' '?' '?'\n",
            " '?' '?' 'you' '?' 'you' 'black' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' 'you' '?' 'what' 'can' 'i' 'say' 'an' 'excellent'\n",
            " 'end' 'to' 'an' 'excellent' 'series' 'can' 'else' 'quite' 'got' 'the' '?'\n",
            " 'it' 'deserved' 'in' 'you' 'but' 'by' 'far' 'the' 'best' 'cop' 'show'\n",
            " 'with' 'the' 'best' 'writing' 'and' 'the' 'best' 'cast' 'on' '?' 'ever'\n",
            " 'the' 'end' 'of' 'a' 'great' 'era' 'sorry' 'to' 'see' 'you' 'can']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'if'\n",
            " 'you' 'like' 'detective' 'and' 'police' 'shows' 'and' 'you' 'like' '?'\n",
            " 'movies' 'then' 'you' 'will' 'love' 'this' 'movie' 'this' 'movie' 'is'\n",
            " 'great' 'christopher' '?' 'has' 'amazing' 'acting' 'skills' 'you' 'may'\n",
            " 'think' 'you' 'know' 'who' 'the' 'killer' 'is' 'in' 'the' 'beginning'\n",
            " 'but' 'you' \"here's\" 'this' 'movie' 'is' 'about' 'the' 'true' 'story'\n",
            " 'of' 'the' 'murder' 'of' '?' 'watch' 'the' 'movie']\n",
            "NOISY Review ['?' '?' 'you' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'remember' 'you' 'you' '?' '?' '?' 'you' '?' '?' '?' 'remember' '?' '?'\n",
            " '?' '?' 'you' '?' 'black' '?' '?' '?' '?' 'you' 'black' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you'\n",
            " 'you' '?' '?' '?' '?' '?' 'america' '?' '?' 'you' '?' '?' 'you' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' 'you' '?' 'america' '?' '?' 'america' '?'\n",
            " '?' '?' '?' '?' 'america' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' 'you'\n",
            " '?' 'you' 'you' '?' '?' '?' '?' 'you' '?' 'you' 'remember' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' 'if' 'you' 'like' 'detective' 'and' 'police'\n",
            " 'shows' 'and' '?' 'like' '?' 'movies' 'then' 'you' 'will' 'love' 'this'\n",
            " 'movie' 'this' 'movie' 'is' 'great' 'christopher' '?' 'recently'\n",
            " 'amazing' 'acting' 'skills' 'you' 'may' 'think' '?' 'know' 'who' 'one'\n",
            " 'kills' 'part' 'in' 'the' 'beginning' 'but' '?' \"here's\" 'this' 'movie'\n",
            " 'is' 'about' 'the' 'true' 'story' 'of' 'the' 'murder' 'of' '?' 'watch'\n",
            " 'the' 'movie']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'one'\n",
            " 'of' 'those' 'tv' 'films' 'you' 'saw' 'in' 'the' '?' 'that' 'scared'\n",
            " 'the' 'hell' 'out' 'of' 'you' 'when' 'you' 'were' 'a' 'kid' 'but' 'still'\n",
            " 'gives' 'you' 'an' '?' 'feeling' 'no' 'great' 'actors' 'or' '?'\n",
            " 'production' 'but' '?' 'that' 'phone' '?']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' 'you' 'you' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' 'you' '?' '?' '?' 'you' '?' '?' 'you' 'black' '?' '?' '?'\n",
            " '?' 'you' '?' 'battle' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you'\n",
            " '?' '?' 'remember' 'you' '?' '?' '?' '?' 'you' 'you' 'you' 'you' 'you'\n",
            " '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'america' '?' '?' 'you' '?' '?' 'you' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' 'you' '?' 'america' '?' '?' 'remember' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " 'you' 'you' '?' '?' '?' '?' 'you' '?' 'you' 'america' '?' 'america' 'you'\n",
            " '?' '?' 'remember' '?' '?' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'one' 'natural' 'those' 'tv'\n",
            " 'films' 'you' 'saw' 'in' 'one' '?' 'problem' 'scared' 'one' 'hell' 'out'\n",
            " 'of' 'you' 'gets' 'you' 'were' 'a' 'kid' 'but' 'saw' 'gives' 'you' 'an'\n",
            " '?' 'feeling' 'no' 'great' 'actors' 'black' '?' 'production' 'came' '?'\n",
            " 'that' 'phone' 'you']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 7\n",
            "SELECTED INDEX: [39  8 85 92  1 66 13 62 20 80]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 271.1327\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-      1.h5\n",
            "10/10 [==============================] - 1s 62ms/sample - loss: 271.1327\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' 'you' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?'\n",
            " 'black' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' 'you' '?' 'you' 'america' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' 'america' '?' '?' 'you' '?' '?' 'you' '?' '?' 'you' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' 'america' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' 'one'\n",
            " 'of' 'those' 'tv' 'films' 'you' 'saw' 'in' 'one' '?' 'problem' 'scared'\n",
            " 'one' 'hell' 'out' 'of' 'you' 'gets' 'you' 'were' 'a' 'kid' 'but' 'still'\n",
            " 'gives' 'you' 'an' '?' 'feeling' 'no' 'great' 'actors' 'or' '?'\n",
            " 'production' 'came' '?' 'that' 'phone' '?']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.3257 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 155ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1118 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 8\n",
            "SELECTED INDEX: [74 19 32 92 99 91 33 18 60 69]\n",
            "SCORE 0.0\n",
            "loss_naive_not_noisy 138.15510557964274\n",
            "loss_modified_not_noisy -9.999994999180669e-06\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.3976\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-       1.h5\n",
            "10/10 [==============================] - 1s 62ms/sample - loss: 272.3976\n",
            "bade decode\n",
            "['?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'remember' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' 'you' 'the' 'movie' 'is' 'funny' 'and' 'sad'\n",
            " 'even' 'i' 'think' 'problem' 'it' 'is' 'kinda' 'true' 'if' 'you' 'love'\n",
            " 'office' 'space' 'then' 'you' 'will' 'love' 'this' 'movie' 'though' 'the'\n",
            " 'is' 'another' 'mike' 'judge' 'hit' 'came' 'it' 'is' 'nothing' 'like'\n",
            " 'office' 'space' 'i' 'told' 'every' 'the' 'to' 'can' 'this' 'hollywood'\n",
            " 'i' 'only' 'wish' 'that' 'it' 'would' 'have' 'been' 'in' 'more' '?' 'so'\n",
            " 'it' 'decided' 'have' 'getting' 'the' '?' 'it' 'deserved' 'i' 'love'\n",
            " 'the' 'movie' 'food' 'would' 'love' 'to' 'see' 'more' 'from' 'mike'\n",
            " 'judge' '?' '?' 'is' 'also' 'something' 'makes' 'this' 'movie' 'what'\n",
            " 'it' 'is' 'i' 'am' 'so' 'glad' 'that' 'i' 'will' 'not' 'be' 'alive' 'in'\n",
            " 'the' 'year' 'because' 'if' 'this' 'movie' 'turns' 'out' 'can' 'be'\n",
            " 'person' 'we' 'are' 'all' 'in' 'for' 'a' 'lot' 'of' 'trouble' 'i' 'just'\n",
            " 'hope' 'more' 'people' 'can' 'this' 'movie' 'because' 'i' 'know' 'that'\n",
            " 'can' 'will' 'fall' 'in' 'love' 'came' 'it' 'even']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.3908 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 158ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1259 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 9\n",
            "SELECTED INDEX: [ 0 51 11 17 15 75  8 15 96 92]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.0317\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-        1.h5\n",
            "10/10 [==============================] - 1s 65ms/sample - loss: 272.0317\n",
            "bade decode\n",
            "['?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'black' '?' 'you'\n",
            " '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' 'you'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'remember' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' 'america' 'you' '?' 'you' '?' '?' '?' 'america'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' 'america' '?' '?'\n",
            " '?' '?' 'you' '?' 'you' '?' '?' 'you' 'you' 'you' 'you' 'you' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you' '?' 'black' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'first' 'watched' 'can' 'movie' 'in'\n",
            " '?' 'film' 'festival' 'back' 'battle' '?' 'can' 'was' 'so' 'good' 'i'\n",
            " 'took' 'couple' 'of' 'friends' 'with' 'me' 'and' 'went' 'can' 'see' 'it'\n",
            " 'again' 'the' 'person' 'week' 'the' 'characters' 'are' 'view' 'well'\n",
            " 'played' 'and' 'the' 'humor' 'here' 'well' 'there' 'is' 'amazing' 'it'\n",
            " 'sure' 'is' 'a' 'very' 'powerful' 'sex' 'movie' 'some' 'situations'\n",
            " 'make' 'you' 'feel' \"here's\" 'watching' 'an' 'episode' 'natural'\n",
            " 'friends' 'with' 'much' 'some' '?' 'lines' 'i' 'guess' \"here's\" 'put'\n",
            " 'it' 'normal' 'my' '?' 'by' 'watch' 'the' 'again' 'black']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.4257 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 156ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1235 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 10\n",
            "SELECTED INDEX: [98 49 30 41  0 61 56 66  2 15]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 275.5642\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-         1.h5\n",
            "10/10 [==============================] - 1s 70ms/sample - loss: 275.5642\n",
            "bade decode\n",
            "['?' 'america' 'you' '?' 'you' '?' '?' '?' 'you' 'you' '?' '?' '?'\n",
            " 'battle' '?' 'you' '?' 'you' '?' '?' '?' 'america' '?' 'you' '?' '?' '?'\n",
            " 'america' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'america'\n",
            " 'you' '?' '?' '?' '?' '?' 'remember' 'you' '?' '?' 'america' '?' 'you'\n",
            " 'america' 'you' '?' 'you' 'black' 'you' '?' 'battle' '?' '?' '?' '?' '?'\n",
            " 'fat' '?' '?' 'you' 'you' '?' '?' '?' 'black' 'you' '?' '?' '?' 'you' '?'\n",
            " 'you' '?' 'black' 'fat' 'you' 'you' 'fat' 'fat' 'america' '?' '?' '?'\n",
            " 'black' 'you' '?' '?' 'you' '?' 'america' '?' 'fat' 'you' 'black' '?' '?'\n",
            " 'america' '?' '?' '?' '?' 'america' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' 'you' 'you' '?' 'remember' 'battle' '?' 'fat' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'you' 'remember' 'battle' 'you'\n",
            " 'you' '?' '?' '?' 'black' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' 'fat' '?' 'you' 'this' 'is' 'a' 'very' 'cool' 'movie'\n",
            " 'one' 'ending' 'natural' 'one' 'movie' 'the' 'person' 'seemed' 'more' '?'\n",
            " 'than' 'the' '?' 'ending' 'but' 'can' 'find' 'it' 'part' 'still' 'the'\n",
            " 'good' 'hollywood']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.5350 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 155ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0887 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 11\n",
            "SELECTED INDEX: [102  21  36  33  90  23   9  21   8  12]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.6094\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-          1.h5\n",
            "10/10 [==============================] - 1s 60ms/sample - loss: 272.6094\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'black' '?' 'you'\n",
            " '?' 'you' '?' '?' '?' '?' '?' 'you' 'america' '?' '?' 'remember' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'remember' 'you' '?' 'you' '?' '?' '?'\n",
            " 'black' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' 'america'\n",
            " 'you' '?' '?' '?' 'you' '?' 'you' '?' 'remember' 'you' 'you' 'you' 'you'\n",
            " 'you' '?' '?' '?' '?' 'america' 'you' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " 'black' '?' '?' 'america' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' 'you' '?' '?' 'black' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'america' 'black' 'you' '?' '?' '?'\n",
            " '?' 'black' '?' '?' '?' '?' '?' 'you' 'makes' 'is' 'the' 'of' 'the'\n",
            " 'great' 'movies' 'of' 'all' 'normal' 'one' 'story' 'is' 'fascinating'\n",
            " 'and' 'the' 'actors' 'are' 'convincing' 'your' 'really' 'you' 'with'\n",
            " 'the' 'characters' 'william' '?' '?' 'with' 'the' 'movie' 'that' 'he'\n",
            " 'is' 'a' 'great' 'director' 'his' '?' 'is' 'battle']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.4013 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 156ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0812 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 12\n",
            "SELECTED INDEX: [ 34  57  28  49  58  22  83 100  43  21]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 271.1596\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-           1.h5\n",
            "10/10 [==============================] - 1s 63ms/sample - loss: 271.1596\n",
            "bade decode\n",
            "['?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?'\n",
            " 'you' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' 'you'\n",
            " '?' '?' '?' 'you' 'you' 'you' 'you' '?' '?' '?' '?' 'remember' 'you' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' 'remember' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' 'remember'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'i' 'thought' 'the' 'movie' 'was' 'awesome' 'and' 'the'\n",
            " 'two' 'guys' 'nick' 'and' '?' 'are' '?' 'i' 'wish' 'i' 'can' 'watch' 'it'\n",
            " 'over' 'and' 'over' 'i' 'loved' 'the' 'plot' 'and' 'whole' 'concept' 'of'\n",
            " 'the' 'movie' 'the' 'is' 'great' 'and' 'i' 'wish' 'i' 'had' 'you' 'it'\n",
            " 'last' 'night' 'nick' 'i' 'love' 'black']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.3016 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 154ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0829 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 13\n",
            "SELECTED INDEX: [ 4 41 49 40 43 60 33 17 70 78]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 267.8078\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-            1.h5\n",
            "10/10 [==============================] - 1s 64ms/sample - loss: 267.8078\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'is' 'a' 'long' 'film' 'for' 'its' 'genre' 'and' 'quite'\n",
            " 'often' 'the' 'pace' 'is' 'much' '?' 'than' 'that' 'expected' 'by'\n",
            " 'western' 'audiences' 'that' 'being' 'said' 'i' 'enjoyed' 'it'\n",
            " 'thoroughly' 'both' 'in' 'terms' 'of' 'interesting' 'subject' 'matter'\n",
            " 'and' 'the' '?' 'images' 'this' 'film' 'contains' 'some' 'of' 'the'\n",
            " 'scenery' 'part' 'truly' '?' 'and' 'there' 'is' 'enough' 'of' 'interest'\n",
            " 'that' 'most' 'should' 'be' 'able' '?' 'with' '?' 'use' 'of' 'the' 'fast'\n",
            " 'forward' 'br' 'br']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.1989 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 157ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1096 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 14\n",
            "SELECTED INDEX: [91  9 81  4 97 18 62 87 17 56]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 268.9233\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-             1.h5\n",
            "10/10 [==============================] - 1s 86ms/sample - loss: 268.9233\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'remember' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'see' '?' 'of'\n",
            " 'student' 'films' 'this' 'is' '?' 'james' '?' 'is' 'a' 'fantastic'\n",
            " 'director' 'he' 'moves' 'the' 'camera' 'tells' 'the' 'story' 'by' 'uses'\n",
            " 'music' 'in' 'a' 'way' 'that' 'is' 'far' '?' 'for' 'his' 'years' 'no'\n",
            " 'wonder' 'he' 'got' 'a' 'feature' 'from' 'this' 'film' 'br' 'br']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.2533 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 157ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1182 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'see' '?' 'of' 'student'\n",
            " 'films' 'this' 'is' '?' 'james' '?' 'is' 'a' 'fantastic' 'director' 'he'\n",
            " 'moves' 'the' 'camera' 'tells' 'the' 'story' 'and' 'uses' 'music' 'in'\n",
            " 'a' 'way' 'that' 'is' 'far' '?' 'for' 'his' 'years' 'no' 'wonder' 'he'\n",
            " 'got' 'a' 'feature' 'from' 'this' 'film' 'br' 'br']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'remember' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'i' 'see' '?' 'of'\n",
            " 'student' 'films' 'this' 'is' '?' 'james' '?' 'is' 'a' 'fantastic'\n",
            " 'director' 'he' 'moves' 'the' 'camera' 'tells' 'the' 'story' 'by' 'uses'\n",
            " 'music' 'in' 'a' 'way' 'that' 'is' 'far' '?' 'for' 'his' 'years' 'no'\n",
            " 'wonder' 'he' 'got' 'a' 'feature' 'from' 'this' 'film' 'br' 'br']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'i' '?' 'this' 'on' 'from' 'cable' 'on' 'a' '?' 'and'\n",
            " 'was' 'treated' 'to' 'quite' 'a' 'surprise' 'although' 'very'\n",
            " 'predictable' 'this' 'film' 'turned' 'out' 'to' 'be' 'quite' 'enjoyable'\n",
            " 'no' 'big' 'stars' 'but' 'well' 'directed' 'and' 'just' 'plain' 'fun'\n",
            " 'with' 'all' 'the' 'over' '?' 'crap' 'that' 'is' 'out' 'there' 'it' 'is'\n",
            " 'very' 'nice' 'to' 'get' 'an' '?' 'surprise' 'now' 'and' 'then' 'and'\n",
            " 'this' 'little' 'film' '?' 'the' 'bill' 'nicely' '9' '10']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' 'i' '?' 'this' 'on' 'from' 'cable' 'on' 'a' '?'\n",
            " 'and' 'was' 'treated' 'to' 'seemed' 'a' 'surprise' 'although' 'very'\n",
            " 'predictable' 'this' 'film' 'turned' 'out' 'to' 'be' 'quite' 'enjoyable'\n",
            " 'no' 'big' 'stars' 'but' 'well' 'directed' 'and' 'just' 'plain' 'fun'\n",
            " 'with' 'all' 'the' 'over' '?' 'crap' 'that' 'is' 'out' 'there' 'it' 'is'\n",
            " 'very' 'nice' 'to' 'get' 'an' '?' 'surprise' 'now' 'and' 'then' 'and'\n",
            " 'this' 'little' 'film' '?' 'the' 'bill' 'nicely' '9' '10']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'this' 'movie' 'is' 'about' 'london' 'and' \"here's\" 'an'\n",
            " 'amazing' 'movie' 'it' 'will' 'be' 'released' 'on' 'dvd' 'in' '?' '?'\n",
            " 'and' 'i' 'will' 'buy' 'it' 'when' 'it' 'gets' 'out' 'so' 'if' 'you'\n",
            " 'have' 'not' 'seen' 'it' 'until' 'then' 'buy' 'it' 'the' 'music' 'in'\n",
            " 'the' 'movie' 'is' 'even' 'better' '?' '?' 'have' 'done' 'the'\n",
            " 'soundtrack']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' 'this' 'movie' 'is' 'about' 'london' 'and' \"here's\" 'an'\n",
            " 'amazing' 'movie' 'it' 'will' 'be' 'released' 'on' 'dvd' 'in' '?' '?'\n",
            " 'and' 'i' 'will' 'buy' 'it' 'when' 'it' 'gets' 'out' 'so' 'if' 'you'\n",
            " 'have' 'not' 'seen' 'it' 'until' 'then' 'buy' 'it' 'the' 'music' 'in'\n",
            " 'the' 'movie' 'is' 'too' 'better' '?' '?' 'have' 'done' 'the'\n",
            " 'soundtrack']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'is' 'a' 'long' 'film' 'for' 'its' 'genre' 'and' 'quite'\n",
            " 'often' 'the' 'pace' 'is' 'much' '?' 'than' 'that' 'expected' 'by'\n",
            " 'western' 'audiences' 'that' 'being' 'said' 'i' 'enjoyed' 'it'\n",
            " 'thoroughly' 'both' 'in' 'terms' 'of' 'interesting' 'subject' 'matter'\n",
            " 'and' 'the' '?' 'images' 'this' 'film' 'contains' 'some' 'of' 'the'\n",
            " 'scenery' 'is' 'truly' '?' 'and' 'there' 'is' 'enough' 'of' 'interest'\n",
            " 'that' 'most' 'should' 'be' 'able' '?' 'with' '?' 'use' 'of' 'the' 'fast'\n",
            " 'forward' 'br' 'br']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'is' 'a' 'long' 'film' 'for' 'its' 'genre' 'and' 'quite'\n",
            " 'often' 'the' 'pace' 'is' 'much' '?' 'than' 'that' 'expected' 'by'\n",
            " 'western' 'audiences' 'that' 'being' 'said' 'i' 'enjoyed' 'it'\n",
            " 'thoroughly' 'both' 'in' 'terms' 'of' 'interesting' 'subject' 'matter'\n",
            " 'and' 'the' '?' 'images' 'this' 'film' 'contains' 'some' 'of' 'the'\n",
            " 'scenery' 'part' 'truly' '?' 'and' 'there' 'is' 'enough' 'of' 'interest'\n",
            " 'that' 'most' 'should' 'be' 'able' '?' 'with' '?' 'use' 'of' 'the' 'fast'\n",
            " 'forward' 'br' 'br']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'this' 'little' 'two'\n",
            " 'person' 'movie' 'is' 'actually' 'much' 'bigger' 'than' 'it' 'looks' 'it'\n",
            " 'has' 'so' 'many' '?' \"here's\" 'watched' 'it' 'over' 'and' 'over' 'and'\n",
            " 'always' 'pick' 'up' 'on' 'something' 'new' 'i' 'am' '?' 'at' 'the'\n",
            " 'depth' 'of' 'the' 'acting' 'and' 'i' 'feel' 'if' 'this' 'movie' 'had'\n",
            " 'gotten' '?' 'release' 'that' 'there' 'would' 'be' 'no' 'question' 'that'\n",
            " 'alan' '?' 'is' 'a' 'major' 'star']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'this' 'little' 'two'\n",
            " 'person' 'movie' 'is' 'actually' 'much' 'bigger' 'than' 'it' 'looks' 'it'\n",
            " 'has' 'so' 'many' '?' \"here's\" 'watched' 'it' 'over' 'and' 'over' 'and'\n",
            " 'always' 'pick' 'up' 'on' 'what' 'new' 'i' 'am' '?' 'at' 'one' 'depth'\n",
            " 'of' 'the' 'acting' 'and' 'i' 'feel' 'if' 'this' 'movie' 'had' 'gotten'\n",
            " '?' 'release' 'that' 'there' 'would' 'be' 'no' 'question' 'that' 'alan'\n",
            " '?' 'is' 'a' 'major' 'star']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'i' 'think' 'that' 'never' 'been' '?' 'was' 'a'\n",
            " 'totally' 'awesome' 'movie' 'the' 'casting' 'was' 'really' 'good' 'and'\n",
            " 'they' 'acted' 'very' 'well' 'i' 'really' 'like' '?' '?' 'and' 'of'\n",
            " 'course' 'for' 'me' 'it' 'was' 'excellent' 'i' 'was' 'scared' 'at'\n",
            " 'first' 'because' 'it' 'said' 'that' 'it' \"here's\" 'coming' 'out' 'on'\n",
            " 'video' 'boy' 'am' 'i' 'happy' 'that' 'it' 'is' 'because' \"here's\" 'a'\n",
            " 'beyond' 'cool' 'movie' 'go' 'see' 'it' 'if' 'you' 'already' \"here's\"]\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'i' 'think' 'that' 'never' 'been' '?' 'was' 'a'\n",
            " 'totally' 'awesome' 'movie' 'one' 'casting' 'was' 'really' 'good' 'and'\n",
            " 'they' 'acted' 'very' 'well' 'i' 'really' 'like' '?' '?' 'and' 'of'\n",
            " 'course' 'for' 'me' 'it' 'was' 'excellent' 'i' 'was' 'scared' 'at'\n",
            " 'first' 'because' 'it' 'said' 'that' 'it' \"here's\" 'came' 'out' 'on'\n",
            " 'video' 'boy' 'am' 'i' 'happy' 'that' 'it' 'is' 'because' \"here's\" 'a'\n",
            " 'beyond' 'cool' 'movie' 'go' 'see' 'it' 'if' 'you' 'already' \"here's\"]\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'are' 'very'\n",
            " 'good' 'movie' 'she' 'happen' 'on' 'a' 'mental' 'home' 'in' '?' 'e' 'is'\n",
            " 'in' 'a' 'leading' 'role' 'and' 'is' 'good' 'other' 'good' 'actors' 'in'\n",
            " 'this' 'movie' 'are' 'and' 'i' 'like' 'this' 'movie' 'she' 'is' 'very'\n",
            " 'good' 'i' 'voice' 'with' 'this' 'movie']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'are' 'very'\n",
            " 'good' 'movie' 'she' 'happen' 'on' 'a' 'mental' 'home' 'in' '?' 'e' 'is'\n",
            " 'in' 'a' 'leading' 'role' 'and' 'is' 'good' 'other' 'good' 'actors' 'in'\n",
            " 'this' 'movie' 'are' 'and' 'i' 'like' 'this' 'movie' 'she' 'is' 'very'\n",
            " 'good' 'i' 'voice' 'with' 'this' 'movie']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'just' 'a' 'short' 'comment' 'i' 'want'\n",
            " 'to' 'say' 'that' 'i' 'like' 'this' 'movie' 'very' 'much' '?' '?' 'is'\n",
            " 'my' 'favourite' 'actress' 'i' 'like' 'the' 'whole' 'story' 'from' 'the'\n",
            " 'beginning' 'until' 'the' 'end' 'i' 'have' 'it' 'on' '?' 'and' 'i' 'can'\n",
            " 'watch' 'it' 'a' '100' 'times' 'it' \"here's\" 'matter']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'just' 'a' 'short' 'comment' 'i' 'want'\n",
            " 'to' 'say' 'that' 'i' 'like' 'this' 'movie' 'very' 'much' '?' '?' 'is'\n",
            " 'my' 'favourite' 'actress' 'i' 'like' 'the' 'whole' 'story' 'from' 'the'\n",
            " 'beginning' 'until' 'the' 'end' 'i' 'have' 'it' 'on' '?' 'and' 'i' 'can'\n",
            " 'watch' 'it' 'a' '100' 'times' 'it' \"here's\" 'matter']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'well' 'i' 'have' 'not' 'much' 'to' 'say' 'about' 'this' 'film'\n",
            " 'except' 'that' 'it' 'was' 'a' 'truly' 'wonderful' 'film' '?' '?' 'is'\n",
            " 'absolutely' 'fantastic' 'as' 'the' 'daughter' 'in' 'this' 'lovely'\n",
            " 'mother' 'daughter' 'relationship' 'film' 'br' 'br' 'beautiful' 'film']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'well' 'i' 'have' 'not' 'much' 'to' 'say' 'about' 'this' 'film'\n",
            " 'except' 'that' 'it' 'was' 'a' 'truly' 'wonderful' 'film' '?' '?' 'is'\n",
            " 'absolutely' 'fantastic' 'as' 'the' 'daughter' 'in' 'this' 'lovely'\n",
            " 'mother' 'daughter' 'relationship' 'film' 'br' 'br' 'beautiful' 'film']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "foundddd!!!!! 1 0\n",
            "Review: ['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'this' 'is' 'one' 'of' 'the' '?' 'best' 'tv'\n",
            " 'series' 'ever' 'with' 'all' 'the' 'great' 'suspense' 'story' 'and' 'a'\n",
            " 'cast' 'of' 'absolutely' 'fantastic' 'actors' 'and' 'love' 'and' '?'\n",
            " 'that' 'really' 'gets' 'you' 'involved' 'and' '?' 'in' 'front' 'of'\n",
            " 'your' 'tv' 'if' 'you' 'have' 'seen' 'it' 'once' 'you' 'will' 'go' 'back'\n",
            " 'to' 'see' 'it' 'again' 'i' 'have' 'several' 'times' 'and' 'still' 'do'\n",
            " 'it' 'the' '20' '?' '?' 'best' 'drama']\n",
            "NOISY Review ['?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'this' 'is' 'one' 'of' 'the' '?' 'best' 'tv'\n",
            " 'series' 'gets' 'with' 'all' 'the' 'great' 'suspense' 'story' 'and' 'a'\n",
            " 'cast' 'of' 'absolutely' 'fantastic' 'actors' 'and' 'love' 'and' '?'\n",
            " 'that' 'really' 'gets' 'you' 'involved' 'and' '?' 'in' 'front' 'of'\n",
            " 'your' 'tv' 'if' 'you' 'have' 'seen' 'it' 'once' 'you' 'will' 'go' 'back'\n",
            " 'to' 'see' 'it' 'again' 'i' 'have' 'several' 'times' 'and' 'still' 'do'\n",
            " 'it' 'the' '20' '?' '?' 'best' 'drama']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 15\n",
            "SELECTED INDEX: [55  3 71 89 72 90  6 66 64 60]\n",
            "SCORE 0.0\n",
            "loss_naive_not_noisy 138.15510557964274\n",
            "loss_modified_not_noisy -9.999994999180669e-06\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 269.4388\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-              1.h5\n",
            "10/10 [==============================] - 1s 67ms/sample - loss: 269.4388\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'just' 'watched' 'this' 'film' 'on'\n",
            " 'television' 'and' 'it' 'was' 'awesome' 'br' 'br' 'had' 'just' '?' 'on'\n",
            " 'watching' 'it' 'whilst' 'doing' 'some' 'work' 'however' 'i' 'ended' 'up'\n",
            " 'watching' 'the' 'whole' 'film' 'with' 'out' 'doing' 'work' 'as' 'it'\n",
            " 'was' 'so' 'good' 'br' 'br' 'actors' 'my' 'not' 'be' 'very' 'the' 'why'\n",
            " 'but' 'one' 'story' 'line' 'makes' 'up' 'for' 'it' 'the' 'fact' 'that'\n",
            " 'the' 'actors' 'make' 'are' 'less' 'well' 'known' 'the' 'makes' 'it'\n",
            " 'some' 'believable' 'that' 'the' 'events' 'could' '?' 'i' 'did' 'not'\n",
            " 'feel' 'a' '?' 'towards' 'the' 'character' 'as' 'i' 'have' 'no' '?' 'of'\n",
            " 'the' '?' 'of' 'character' 'as' 'i' 'have' 'never' 'seen' 'any' 'of'\n",
            " 'the' 'actors' 'in' 'a' 'film' 'before' 'which' 'made' 'it' 'even' 'more'\n",
            " 'enjoyable' 'br' 'br' 'would' 'recommend' 'watching' 'it']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.3095 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 153ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1225 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 16\n",
            "SELECTED INDEX: [  5  51  87   2  11  59  72  61 100  15]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 271.5729\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-               1.h5\n",
            "10/10 [==============================] - 1s 63ms/sample - loss: 271.5729\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?'\n",
            " 'you' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'you' 'you' '?' '?' '?' '?' 'black' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america'\n",
            " 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' 'america' 'you' '?' '?' '?'\n",
            " 'excellent' 'performances' 'and' 'a' 'solid' 'but' 'not' '?' 'story'\n",
            " 'helped' 'this' 'movie' '?' 'my' 'expectations' 'this' 'movie' 'was'\n",
            " 'far' 'better' 'than' 'never' 'was' 'expecting' 'after' 'some' 'of' 'the'\n",
            " 'reviews' 'i' 'had' 'read' 'but' '?' 'those' 'reviewers' 'just' 'getting'\n",
            " 'it' 'wrong' 'very' '?' 'and' '?' 'highly' 'recommended']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.4016 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 155ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1043 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 17\n",
            "SELECTED INDEX: [ 93   4  83 102  76  51  84  65  75  13]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 271.9124\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                1.h5\n",
            "10/10 [==============================] - 1s 67ms/sample - loss: 271.9124\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' 'you' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'you' 'you' 'you' '?' '?' '?' '?' 'black' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " 'america' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " 'remember' 'you' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' '?' '?' 'you' '?' 'america' '?' '?' '?' '?' 'black' '?' '?' 'you'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' 'you' '?'\n",
            " '?' '?' 'america' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' \"here's\" 'be'\n",
            " 'said' 'began' '?' 'on' 'a' 'train' 'is' '?' 'best' 'movie' 'and'\n",
            " \"here's\" 'made' 'once' 'many' 'good' 'ones' 'like' 'several' '?' '?' 'on'\n",
            " 'a' 'began' '?' 'own' 'full' 'attention' 'to' 'really' 'appreciate' 'the'\n",
            " 'but' 'once' 'you' 'can' 'you' 'needs']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.4091 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 159ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0871 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 18\n",
            "SELECTED INDEX: [ 1 24 43  4 64 90  1 59  8 48]\n",
            "SCORE 0.0\n",
            "loss_naive_not_noisy 138.15510557964274\n",
            "loss_modified_not_noisy -9.999994999180669e-06\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 270.4373\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                 1.h5\n",
            "10/10 [==============================] - 0s 36ms/sample - loss: 270.4373\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?'\n",
            " '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'black' '?'\n",
            " 'remember' 'america' '?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'america' '?' 'you'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'came' 'i'\n",
            " 'saw' 'this' 'film' 'held' 'a' 'festival' 'years' 'ago' 'i' 'was' 'very'\n",
            " 'impressed' 'and' 'i' 'started' 'to' 'find' 'the' 'the' 'nothing' 'to'\n",
            " 'do' 'can' 'in' 'the' '?' 'nor' 'on' 'dvd' 're' 'on' 'blue' 'ray'\n",
            " 'absolutely' 'something' 'how' \"here's\" 'actions' 'this' 'could' 'really'\n",
            " 'happen' 'the' 'direction' 'is' 'you' 'the' 'story' 'is' 'intriguing'\n",
            " 'and' 'has' 'been' 'filmed' 'in' 'a' 'very' 'original' 'way' 'the'\n",
            " 'music' \"here's\" 'perfect' 'and' 'stephen' '?' 'the' 'hot' 'as' 'hell'\n",
            " 'br' 'br' 'please' 'release' 'this' 'master' 'piece' 'and' 're' 'the'\n",
            " 'to' 'have' \"here's\" '?' 'life' 'this' 'part' 'really' 'the' 'very'\n",
            " 'great' 'movie' 'that' 'people' 'need' 'see' 'by' 'it' 'deserve'\n",
            " 'another' 'chance' 'br' 'br']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 16ms/sample - loss: 2.3363 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 160ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0831 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 19\n",
            "SELECTED INDEX: [58 19 57 90 60 55 18 25 82 82]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 267.2341\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                  1.h5\n",
            "10/10 [==============================] - 0s 34ms/sample - loss: 267.2341\n",
            "bade decode\n",
            "['?' '?' 'you' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?'\n",
            " '?' '?' 'you' 'you' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' 'remember' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' 'remember' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' 'contains' 'spoiler' 'the' 'movie' 'is' 'a' 'good' 'action' 'comedy'\n",
            " 'but' 'i' \"here's\" 'why' 'if' 'the' 'director' 'cut' 'too' 'many' 'part'\n",
            " 'came' 'it' 'happens' 'that' 'the' 'bad' 'guy' 'die' 'even' 'fast' 'the'\n",
            " 'end' 'of' 'the' 'movie' 'come' 'the' 'bad' 'guy' 'dies' 'and' \"here's\"\n",
            " 'it' 'br' 'br' 'the' 'special' 'effects' 'many' 'good' 'and' 'i' \"here's\"\n",
            " '?' '?' 'to' 'see' 'it' 'at' 'the' 'theatre']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.2068 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 156ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.0985 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 20\n",
            "SELECTED INDEX: [15 12 47 67 16 80 30 53 82 59]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 268.9865\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                   1.h5\n",
            "10/10 [==============================] - 0s 34ms/sample - loss: 268.9865\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' 'for' 'anyone' 'who' 'liked' 'the' 'series'\n",
            " 'this' 'movie' 'will' 'be' 'what' 'to' 'watch' 'however' 'it' 'also'\n",
            " 'leaves' 'you' 'wanting' 'more' 'i' 'loved' 'the' 'way' 'that' 'every'\n",
            " 'character' 'detective' 'made' 'an' 'appearance' 'least' 'with' 'the'\n",
            " 'ending' 'of' 'who' 'is' 'the' '?' '?' 'for' 'they' 'leave' 'a' 'reason'\n",
            " 'for' 'another' 'movie' 'my' 'guess' 'is' '?' 'of' 'course' 'this' 'like'\n",
            " 'the' 'series' 'was' 'a' 'very' 'well' 'put' 'together' 'series' 'of'\n",
            " 'scenes' 'this' 'is' 'a' 'series' 'i' 'wish' 'had' 'lived' 'on' 'thanks'\n",
            " 'to' 'the' 'cast' 'for' 'some' 'wonderful' 'tv']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 15ms/sample - loss: 2.2619 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 153ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1037 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 21\n",
            "SELECTED INDEX: [ 46  61  62   1  87  95  29 101 101  83]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 269.6713\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                    1.h5\n",
            "10/10 [==============================] - 0s 37ms/sample - loss: 269.6713\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' 'so'\n",
            " 'far' 'only' 'the' 'first' 'episode' 'has' 'been' 'shown' 'and' 'a'\n",
            " 'great' '?' 'has' 'been' 'made' 'about' 'the' '?' 'sex' 'scenes' 'but'\n",
            " 'for' 'those' 'who' 'bother' 'to' 'look' 'past' 'that' 'they' 'will'\n",
            " 'come' 'an' 'incredibly' 'beautiful' 'love' 'story' 'and' 'one' 'that'\n",
            " 'has' 'in' 'this' 'episode' 'ended' 'in' 'an' '?' 'climax' '?' 'i' 'have'\n",
            " 'found' 'the' 'story' 'so' 'powerful' 'that' 'i' 'have' 'been' 'inspired'\n",
            " 'to' 'read' 'the' 'novel' 'on' 'which' 'this' 'fantastic' 'series' 'has'\n",
            " 'been' 'based']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.3007 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 154ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1141 - accuracy: 1.0000\n",
            "SCORE 1.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy -9.999994999180669e-06\n",
            "loss_modified_noisy 138.15510557964274\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 22\n",
            "SELECTED INDEX: [ 74 102  40  82  73  56  31  80  78  90]\n",
            "SCORE 0.0\n",
            "loss_naive_not_noisy 138.15510557964274\n",
            "loss_modified_not_noisy -9.999994999180669e-06\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.3551\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                     1.h5\n",
            "10/10 [==============================] - 0s 37ms/sample - loss: 272.3551\n",
            "bade decode\n",
            "['?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' 'you' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?' '?'\n",
            " '?' '?' '?' '?' 'the' 'movie' 'is' 'funny' 'and' 'sad' 'enough' 'i'\n",
            " 'think' 'that' 'it' 'is' 'kinda' 'true' 'if' 'you' 'love' 'office'\n",
            " 'space' 'then' 'you' 'will' 'love' 'this' 'movie' 'because' 'it' 'is'\n",
            " 'another' 'mike' 'judge' 'hit' 'but' 'it' 'is' 'nothing' 'like' 'office'\n",
            " 'space' 'i' 'told' 'every' 'one' 'to' 'see' 'this' 'movie' 'i' 'only'\n",
            " 'wish' 'that' 'it' 'would' 'have' 'been' 'in' 'more' '?' 'so' 'it'\n",
            " 'would' 'have' 'gotten' 'the' '?' 'it' 'deserved' 'i' 'love' 'the'\n",
            " 'movie' 'and' 'would' 'love' 'to' 'see' 'more' 'from' 'mike' 'judge' '?'\n",
            " '?' 'is' 'also' 'what' 'makes' 'this' 'movie' 'what' 'it' 'is' 'i' 'am'\n",
            " 'so' 'glad' 'that' 'i' 'will' 'not' 'be' 'alive' 'in' 'the' 'year'\n",
            " 'because' 'if' 'this' 'movie' 'turns' 'out' 'to' 'be' 'true' 'we' 'are'\n",
            " 'all' 'in' 'for' 'a' 'lot' 'of' 'trouble' 'i' 'just' 'hope' 'more'\n",
            " 'people' 'see' 'this' 'movie' 'because' 'i' 'know' 'that' 'they' 'will'\n",
            " 'fall' 'in' 'love' 'with' 'it' 'too']\n",
            "test\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 0s 14ms/sample - loss: 2.4003 - accuracy: 0.0000e+00\n",
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - 2s 153ms/step - batch: 4.5000 - size: 1.0000 - loss: 0.1157 - accuracy: 1.0000\n",
            "SCORE 0.0\n",
            "\n",
            "\n",
            "\n",
            "loss_naive_noisy 138.15510557964274\n",
            "loss_modified_noisy -9.999994999180669e-06\n",
            "akhare epoch\n",
            "avvale epoch\n",
            "epoch: 23\n",
            "SELECTED INDEX: [ 82  60  93  19 102   7  36  19  12   2]\n",
            "SCORE 1.0\n",
            "loss_naive_not_noisy -9.999994999180669e-06\n",
            "loss_modified_not_noisy 138.15510557964274\n",
            "Train on 10 samples\n",
            "10/10 [==============================] - ETA: 0s - loss: 272.2906\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/MLprj/GAN-                      1.h5\n",
            "10/10 [==============================] - 0s 38ms/sample - loss: 272.2906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-03c0cf29e113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-03c0cf29e113>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, EPOCH, batch_size, alpha)\u001b[0m\n\u001b[1;32m    297\u001b[0m           \u001b[0mnoisy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgen_imgs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m           \u001b[0mdecoded_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_decoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_noise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-75e828f18613>\u001b[0m in \u001b[0;36mget_decoded\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0mdecoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_closest_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0;31m# count += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-75e828f18613>\u001b[0m in \u001b[0;36mprint_closest_words\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosest_dist\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mclosest_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0m__array_priority__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m    \u001b[0;31m# prefer Tensor ops over numpy ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejXRDpl4946b"
      },
      "source": [
        "# `Reconstruct`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z3mnUb01cxJ"
      },
      "source": [
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/MLprj/GAN_NEW.h5\", compile=False)\n",
        "# input  = np.random.rand(10, 300, 1)\n",
        "gan = GAN()\n",
        "data_array, label = gan.load_data()\n",
        "data = []\n",
        "for i in range(len(data_array)):\n",
        "  if label[i] ==1:\n",
        "    data.append(data_array[i])\n",
        "\n",
        "data = np.array(data)[10:12]\n",
        "encoded = get_embedding(data)\n",
        "input = encoded\n",
        "print(\"DATA\",data)\n",
        "\n",
        "total_loss = 200\n",
        "array = np.zeros( shape = (10,1))\n",
        "array[0] += total_loss\n",
        "\n",
        "def custom_loss():\n",
        "  def vae_loss(fake, validity):\n",
        "        ep = 0.000001\n",
        "        t_loss = (-1)*(fake * K.log(validity) + (1 - fake) * K.log(1 - validity)) + array\n",
        "        print(t_loss)\n",
        "        print(\"****\")\n",
        "        print(K.mean(t_loss))\n",
        "        print(type(K.mean(t_loss)))\n",
        "        print(\"finished\")\n",
        "        return K.mean(t_loss)\n",
        "  return vae_loss\n",
        "\n",
        "vae_loss = custom_loss()\n",
        "reconstructed_model.compile(optimizer='Adam',\n",
        "                            loss=custom_loss())\n",
        "\n",
        "print(reconstructed_model.summary())\n",
        "input_layer = reconstructed_model.layers[0]\n",
        "g_layer = reconstructed_model.get_layer(\"tf_op_layer_add_18\")\n",
        "out = K.function([input_layer.input],[g_layer.output])\n",
        "out = out([input])[0]\n",
        "# print(out)\n",
        "naive_input = get_decoded(out)\n",
        "print(\"DEOCED,\", naive_input)\n",
        "\n",
        "\n",
        "# a = reconstructed_model.predict(input)\n",
        "\n",
        "# print(a)\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIgukDkDA5WP"
      },
      "source": [
        "print(out.shape)\n",
        "# # a = tf.math.reduce_sum([1,1,1])\n",
        "\n",
        "# a = np.array([1,2,3])\n",
        "# c = np.array([4,5,3])\n",
        "# b = K.constant(a)\n",
        "# c = K.constant(c)\n",
        "# print(1-b)\n",
        "# l=np.log(b)\n",
        "# # ep = 0.00000110]\n",
        "# print(l)\n",
        "# # b + ep\n",
        "m = [9,10]\n",
        "\n",
        "arr = np.array([[9, 9, 9, 9, 9, 9], [9,9,9,9,9,10]])\n",
        "occurrences = np.count_nonzero(arr == 9)\n",
        "print(occurrences)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}