# Adversarial-Attacks-on-Language-Models-Using-Text-GAN

The autoencoder code is inspired from: http://alexadam.ca/ml/2017/05/05/keras-vae.html

References
[1]    Martin Arjovsky, Soumith Chintala, and L ́eon Bottou.Wasserstein GAN. 2017. arXiv:1701.07875 [stat.ML].
[2]    Corinna Cortes and Vladimir Vapnik. “Support-vector networks”. In:Machine Learning20.3(Sept. 1995), pp. 273–297.issn: 1573-0565.doi:10.1007/BF00994018.url:https://doi.org/10.1007/BF00994018.
[3]    David Donahue and Anna Rumshisky.Adversarial  Text  Generation  Without  ReinforcementLearning. 2019. arXiv:1810.06640 [cs.CL].
[4]    Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.Explaining and Harnessing Ad-versarial Examples. 2015. arXiv:1412.6572 [stat.ML].
[5]    Ian J. Goodfellow et al.Generative Adversarial Networks. 2014. arXiv:1406.2661 [stat.ML].
[6]    Sepp Hochreiter and J ̈urgen Schmidhuber. “Long Short-Term Memory”. In:Neural Comput.9.8 (Nov. 1997), pp. 1735–1780.issn: 0899-7667.doi:10.1162/neco.1997.9.8.1735.url:https://doi.org/10.1162/neco.1997.9.8.1735.
[7]IMDB Dataset of 50K Movie Reviews. 2019.url:https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews.
[8]Intuitive Guide to Understanding GloVe Embeddings. 2019.url:https://towardsdatascience.com/light- on- math- ml- intuitive- guide- to- understanding- glove- embeddings-b13b4f19c010.
[9]    Christopher D. Manning Jeffrey Pennington Richard Socher.GloVe: Global Vectors for WordRepresentation. 2014.url:https://nlp.stanford.edu/projects/glove/.
[10]    Diederik P Kingma and Max Welling.Auto-Encoding Variational Bayes. 2014. arXiv:1312.6114 [stat.ML].18
[11]    Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with DeepConvolutional  Neural  Networks”.  In:Advances  in  Neural  Information  Processing  Systems.Ed. by F. Pereira et al. Vol. 25. Curran Associates, Inc., 2012.url:https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
[12]    Yann  LeCun,  Yoshua  Bengio,  and  Geoffrey  Hinton.  “Deep  learning”.  In:Nature521.7553(May 2015), pp. 436–444.issn: 1476-4687.doi:10.1038/nature14539.url:https://doi.org/10.1038/nature14539.
[13]    Bin Liang et al. “Deep Text Classification Can be Fooled”. In:CoRRabs/1704.08006 (2017).arXiv:1704.08006.url:http://arxiv.org/abs/1704.08006.
[14]    Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “” Why should i trust you?” Ex-plaining the predictions of any classifier”. In:Proceedings  of  the  22nd  ACM  SIGKDD  inter-national conference on knowledge discovery and data mining. 2016, pp. 1135–1144.
[15]    Tim  Salimans  et  al.Improved  Techniques  for  Training  GANs.  2016.  arXiv:1606 . 03498[cs.LG].
[16]    M. Schuster and K. K. Paliwal. “Bidirectional recurrent neural networks”. In:IEEE  Trans-actions on Signal Processing45.11 (1997), pp. 2673–2681.doi:10.1109/78.650093.
[17]    Christian Szegedy et al. “Intriguing properties of neural networks”. In:International Confer-ence on Learning Representations. 2014.url:http://arxiv.org/abs/1312.6199.
[18]    Chaowei Xiao et al. “Generating Adversarial Examples with Adversarial Networks”. In:CoRRabs/1801.02610 (2018). arXiv:1801.02610.url:http://arxiv.org/abs/1801.02610.
[19]    Wei Emma Zhang et al.Adversarial  Attacks  on  Deep  Learning  Models  in  Natural  LanguageProcessing: A Survey. 2019. arXiv:1901.06796 [cs.CL].
[20]    Zhengli  Zhao,  Dheeru  Dua,  and  Sameer  Singh.Generating  Natural  Adversarial  Examples.2018. arXiv:1710.11342 [cs.LG].
